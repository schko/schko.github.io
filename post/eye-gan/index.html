<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>style and content transfer of eye images using gans - Machine Learning and Video</title>
  

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="Munna" />
  <meta name="description" content="How discriminator losses and normalization can be used to transfer semantic content and style information. A summary of work by Buhler et al, 2019.
" />

  <meta name="keywords" content="video, creative, agency, production, machine learning" />






<meta name="generator" content="Hugo 0.62.2" />


<link rel="canonical" href="http://schko.github.io/post/eye-gan/" />





<link rel="icon" href="/favicon.ico" />











<link rel="stylesheet" href="/sass/jane.min.0995afa14b62cd93e93cfc066b646c4c17a3eddca0e9d52a1d9dcf5d90aaacd3.css" integrity="sha256-CZWvoUtizZPpPPwGa2RsTBej7dyg6dUqHZ3PXZCqrNM=" media="screen" crossorigin="anonymous">





<meta property="og:title" content="style and content transfer of eye images using gans" />
<meta property="og:description" content="How discriminator losses and normalization can be used to transfer semantic content and style information. A summary of work by Buhler et al, 2019." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://schko.github.io/post/eye-gan/" />
<meta property="article:published_time" content="2021-01-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-01-05T00:00:00+00:00" />
<meta itemprop="name" content="style and content transfer of eye images using gans">
<meta itemprop="description" content="How discriminator losses and normalization can be used to transfer semantic content and style information. A summary of work by Buhler et al, 2019.">
<meta itemprop="datePublished" content="2021-01-05T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2021-01-05T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="2202">



<meta itemprop="keywords" content="eye tracking,gan," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="style and content transfer of eye images using gans"/>
<meta name="twitter:description" content="How discriminator losses and normalization can be used to transfer semantic content and style information. A summary of work by Buhler et al, 2019."/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->




</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">S(M)K</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="http://schko.github.io/">home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="http://schko.github.io/about/about/">about me</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="http://schko.github.io/tags/">tags</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://www.fractal.nyc" rel="noopener" target="_blank">
              fractal media
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          
        
      </li>
    

    
  </ul>
</nav>


  
    






  <link rel="stylesheet" href="/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

  

  

  

  <header id="header" class="header container">
    <div class="logo-wrapper">
  <a href="/" class="logo">
    
      S(M)K
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="http://schko.github.io/">home</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="http://schko.github.io/about/about/">about me</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="http://schko.github.io/tags/">tags</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://www.fractal.nyc" rel="noopener" target="_blank">
              fractal media
              
              <i class="iconfont">
                <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M623.36 272.96 473.216 423.04C467.2 429.056 467.072 438.656 472.896 444.416c0 0-6.72-6.656 1.6 1.6C496.064 467.648 528.64 500.224 528.64 500.224 534.464 506.048 544 505.856 550.016 499.904l150.08-150.144 67.328 66.432c9.024 8.96 27.456 4.544 30.4-8.96 19.968-92.608 46.656-227.52 46.656-227.52 6.848-34.496-16.192-56.704-49.92-49.92 0 0-134.656 26.816-227.328 46.784C560.32 178.048 556.352 182.272 554.752 187.136c-3.2 6.208-3.008 14.208 3.776 20.992L623.36 272.96z"></path>
  <path d="M841.152 457.152c-30.528 0-54.784 24.512-54.784 54.656l0 274.752L237.696 786.56 237.696 237.696l206.016 0c6.656 0 10.752 0 13.248 0C487.68 237.696 512 213.184 512 182.848 512 152.32 487.36 128 456.96 128L183.04 128C153.216 128 128 152.576 128 182.848c0 3.136 0.256 6.272 0.768 9.28C128.256 195.136 128 198.272 128 201.408l0 639.488c0 0.064 0 0.192 0 0.256 0 0.128 0 0.192 0 0.32 0 30.528 24.512 54.784 54.784 54.784l646.976 0c6.592 0 9.728 0 11.712 0 28.736 0 52.928-22.976 54.464-51.968C896 843.264 896 842.304 896 841.344l0-20.352L896 561.408 896 512.128C896 481.792 871.424 457.152 841.152 457.152z"></path>
</svg>

              </i>
            </a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

  </header>

  <div id="mobile-panel">
    <main id="main" class="main bg-llight">
      <div class="content-wrapper">
        <div id="content" class="content container">
          <article class="post bg-white">
    
    <header class="post-header">
      <h1 class="post-title">style and content transfer of eye images using gans</h1>
      
      <div class="post-meta">
        <time datetime="2021-01-05" class="post-time">
          2021-01-05
        </time>
        
        

        
        

        
        
      </div>
    </header>

    
    

    
    <div class="post-content">
      <p>How discriminator losses and normalization can be used to transfer semantic content and style information. A summary of work by Buhler et al, 2019.</p>

<p>The following is a snippet from my first doctoral exam.</p>

<h1 id="background">Background</h1>

<p>A particular challenge in gaze prediction is a lack of data to apply sophisticated models that may be better able to model eye behavior. In fact, lack of high quality eye tracking data is a problem in the field of HCI <sup class="footnote-ref" id="fnref:vanRenswoude2018Gazepath-Quality"><a class="footnote" href="#fn:vanRenswoude2018Gazepath-Quality">1</a></sup><sup class="footnote-ref" id="fnref:Blignaut2014Eye-trackingDesign"><a class="footnote" href="#fn:Blignaut2014Eye-trackingDesign">2</a></sup>, presented by individual variability, privacy concerns and noise in signals from sensors that may be more ubiquitious (e.g., laptop cameras) but imprecise compared to more reliable, in-lab setups (e.g., with IR sensors). Buhler et al<sup class="footnote-ref" id="fnref:buhleretal"><a class="footnote" href="#fn:buhleretal">3</a></sup> address this problem through an application of GANs capable of generating data that maintain both the semantic and style information contained in images of the eye. The authors aim to generate eye image data that preserves the semantic segmentation of eye features (i.e. pupil, iris, sclera) relevant to gaze prediction while also modeling participant-specific eye style, characterized by perceptual features such as skin quality around the eye or sclera vasculature.</p>

<p>The main problem the authors address is the lack of 2D eye images, collected from infrared cameras sensors, for model training. Specifically, they sought to improve data volume for training gaze estimation models under occlusion in VR, synthesize person-specific eye images that satisfy a segmentation mask and follow the style of a specified person from only a few reference images. Furthermore, the authors propose a method to inject style and content information at scale, which has implications past HCI to clinical diagnoses using eye behavior markers in patients with scleritis, autism or schizophrenia <sup class="footnote-ref" id="fnref:Corvera1973TheTest"><a class="footnote" href="#fn:Corvera1973TheTest">4</a></sup><sup class="footnote-ref" id="fnref:Holzman1973Eye-TrackingSchizophrenia"><a class="footnote" href="#fn:Holzman1973Eye-TrackingSchizophrenia">5</a></sup><sup class="footnote-ref" id="fnref:Guillon2014VisualStudies"><a class="footnote" href="#fn:Guillon2014VisualStudies">6</a></sup>.</p>

<p>The authors modify GANs, adversarial networks which typically take random noise as input. By introducing noise, the model can produce data from a variety of locations in the target distribution, subsequently transforming the noise into a meaningful output. In implementation, GANs utilize two loss functions: one that is designed to replicate a probability distribution through generation of new image instances, hence &quot;generative,&quot; and another that acts as a discriminator which outputs a probability of the new image instances being real. Because the authors work relies on extensive technical background, the focus of this brief will cover the design of discriminator loss functions to quantify segmentation and style difference between &quot;fake&quot; and &quot;real&quot; images and the use of normalization to generate convincing fake images. These are critical to understanding recent and future applications of GANs to clinical image data, specifically in the generation of artificial eye images through the authors' proposed Seg2Eye network.</p>

<h2 id="preserving-eye-image-style-through-losses-and-normalization">Preserving eye image style through losses and normalization</h2>

<p>First we define the simple way in which style information difference is quantified across two images by the discriminator. The output of a kernel (i.e., filter) applied to a convolutional layer (<span  class="math">\(l\)</span>) forms the feature map (<span  class="math">\(F^l\)</span>) associated with the layer. The Gram matrix <sup class="footnote-ref" id="fnref:Gatys2016AStyle"><a class="footnote" href="#fn:Gatys2016AStyle">7</a></sup> is computed using data from specific layers and on a number of feature maps (<span  class="math">\(N_l\)</span>). It is a quantification of the similarity between features in a layer, across all feature maps. The <span  class="math">\(N^l \times N^l\)</span> Gram matrix is composed of elements indexed by maps <span  class="math">\(i, j\)</span>:</p>

<p><span  class="math">\[
\mathbf{G}^{l}=\left\langle\mathbf{F}_{i:}^{l}, \mathbf{F}_{j:}^{l}\right\rangle = \left[\begin{array}{c}
\mathbf{F}_{1:}^{T} \\
\mathbf{F}_{2:}^{l}{ }^{T} \\
\vdots \\
\mathbf{F}_{N_{l}:}^{l}
\end{array}\right]\left[\begin{array}{llll}
\mathbf{F}_{1:}^{l} & \mathbf{F}_{2:}^{l} & \cdots & \mathbf{F}_{N_{l}}^{l}
\end{array}\right]
\]</span></p>

<p>where <span  class="math">\(\mathbf{F}_{i:}\)</span> represents the column representation of a feature map <span  class="math">\(i\)</span>. The Gram matrix has been effectively used to quantify the amount of style loss of a generated image <span  class="math">\(\hat{I}\)</span> with respect to a style target image <span  class="math">\(I\)</span>. The style-specific loss <span  class="math">\(\mathcal{L}_{\text {Gram}}\)</span> is calculated by Buhler et al as:</p>

<p><span  class="math">\[
\mathcal{L}_{\text{Gram}}=\Sigma_{i=2}^{m}\left\|\mathbf{G}^{l}\left(F_{E}(\hat{I})\right)-\mathbf{G}^{l}\left(F_{E}(I)\right)\right\|_{1}
\]</span></p>

<p>where <span  class="math">\(E\)</span> is a style encoder trained to discriminate between feature map activations of (real) style images and m is the number of feature maps in <span  class="math">\(E\)</span>. The style loss metric computes the L1 distance (i.e. sum of absolute differences) between a generated and target image feature maps at the Seg2Eye discriminator layer <span  class="math">\(l\)</span>. Thus, the style encoder is trained to distinguish between feature map activations of different styles of eye images, and this information is used in place of calculating the Gram matrix from feature maps within the Seg2Eye network downstream.</p>

<p>Another key aspect of Seg2Eye is the use of adaptive instance normalization (AdaIN) by the generator, which replaces batch normalization typically used in deep networks for the purposes of style transfer. While batch normalization has arguably been one of the core advancements that has made deep learning feasible for a wide range of biomedical problems <sup class="footnote-ref" id="fnref:Santurkar2019HowTraining"><a class="footnote" href="#fn:Santurkar2019HowTraining">8</a></sup>, normalizing batches of data loses style information. AdaIN uses style input to align the channel-wise mean and variance of a content input to match the style. In Seg2Eye, this is used after a convolutional network layer to normalize the generated image statistics using the style reference from a human's eye (Fig. 2a in <sup class="footnote-ref" id="fnref:Buhler2019Content-ConsistentStyle"><a class="footnote" href="#fn:Buhler2019Content-ConsistentStyle">9</a></sup>), transferring qualities such as sclera perforations and skin around the eye to the generated image. For a given style reference image, AdaIN uses its layer activations <span  class="math">\(r\)</span> and computes its mean <span  class="math">\(\mu\)</span> and variance <span  class="math">\(\sigma\)</span> from a layer:</p>

<p><center></p>

<p><figure><img src="/img/buhler2a.png" alt="Figure 2a from Buhler et al" title="Figure 2a from the original paper: Seg2Eye architecture. Describes the authors' novel SPADE+Style Block, which combines Adaptive Instance Normalization (AdaIN) and Spatially Adaptive Normalization (SPADE) to allow for simultaneous style and content injection into the generator at multiple scales."><figcaption>Figure 2a from the original paper: Seg2Eye architecture. Describes the authors' novel SPADE+Style Block, which combines Adaptive Instance Normalization (AdaIN) and Spatially Adaptive Normalization (SPADE) to allow for simultaneous style and content injection into the generator at multiple scales.</figcaption></figure></p>

<p></center></p>

<p><span  class="math">\[
\mu_{n c}(r)=\frac{1}{H W} \sum_{h=1}^{H} \sum_{w=1}^{W} r_{n c h w} \\
\sigma_{n c}(r)=\sqrt{\frac{1}{H W} \sum_{h=1}^{H} \sum_{w=1}^{W}\left(r_{n c h w}-\mu_{n c}(x)\right)^{2}+\epsilon}
\]</span></p>

<p>where <span  class="math">\(H, W\)</span> represent the dimensions of the activations and <span  class="math">\(n, c\)</span> represent the batch and channel (i.e. layer) respectively. Then, for a generated image, the AdaIN block uses the <span  class="math">\(\mu(r)\)</span> and <span  class="math">\(\sigma(r)\)</span> when scaling activations during learning:</p>

<p><span  class="math">\[
\operatorname{AdaIN}(x, r)=\sigma(r)\left(\frac{x_{nc}-\mu(x)}{\sigma(x)}\right)+\mu(r)
\]</span></p>

<p>where <span  class="math">\(x\)</span> refers to the normalized image activations associated with the generated image. AdaIN allows each sample to be normalized distinctly (unlike batch normalization), and furthermore utilizes first- and second-order statistics of preferred activations from the reference style image. Through shifting and scaling layer activations of the generated image via AdaIN and minimizing a loss function (<span  class="math">\(\mathcal{L}_{\text {Gram}}\)</span>) which compares feature maps between the generated and reference image, the network maintains the content's spatial information but uses the human eye reference image to infer style information critical to preserving perceptual similarity of the eye. This was an important goal for the authors to improve data generation methods.</p>

<h2 id="preserving-eye-image-segmentations-through-losses-and-normalization">Preserving eye image segmentations through losses and normalization</h2>

<p>A notable aspect of Seg2Eye is its input: rather than random noise typical of GANs, it utilizes more contextual information in the form of a segmentation mask. In this case, the reference image of the map consisted of a pupil, sclera and iris segmentations. While the Gram matrix is useful for quantifying the difference in consistency of feature maps across images by the discriminator, simply taking the distance between feature maps allows us to learn broad content information contained in an image through a previously proposed <sup class="footnote-ref" id="fnref:Gatys2016AStyle"><a class="footnote" href="#fn:Gatys2016AStyle">10</a></sup> loss function:</p>

<p><span  class="math">\[
\mathcal{L}_{\text{\mathcal{D}_F}}=\Sigma_{i=2}^{m}\left\|F_{D}^{l}(\hat{I})-F_{D}^{l}(I)\right\|_{1}
\]</span></p>

<p>where <span  class="math">\( F_{D}^{l} \)</span> represents the feature map at discriminator network <span  class="math">\(D\)</span> layer <span  class="math">\(l\)</span>.</p>

<p>Using a segmentation mask as input allows the generator to learn context, and requires a solution that achieves the opposite goal of AdaIN. Semantic segmentation treats multiple objects of the same class (e.g. eye segments) as a single entity (&quot;pupil&quot;). On the other hand, instance segmentation treats multiple objects of the same class (person-specific eye segmentation) as distinct individual objects (&quot;pupil 1,&quot; &quot;pupil 2&quot;). The authors use spatially adaptive denormalization (SPADE) to allow for content injection through modulating layer activations from a generated eye image using a segmentation map input <span  class="math">\(m\)</span>. In effect, this translates to adjusting Eqs. 3 and 4 from AdaIN to makes channel-wise adjustments, separately for each layer:</p>

<p><span  class="math">\[
\mu_{c}(x)=\frac{1}{N H W} \sum_{h=1}^{N} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{n c h w} \\
\sigma_{c}(x)=\sqrt{\frac{1}{N H W} \sum_{h=1}^{N} \sum_{h=1}^{H} \sum_{w=1}^{W}\left(x_{n c h w}-\mu_{c}(x)\right)^{2}+\epsilon}
\]</span></p>

<p><span  class="math">\[
\operatorname{SPADE}(x, m)=\gamma_{c h w}(m)\left(\frac{x_{nchw}-\mu(x)}{\sigma(x)}\right)+\beta_{c h w}(m)
\]</span></p>

<p>where <span  class="math">\(\mu\)</span> and <span  class="math">\(\sigma\)</span> are now computed across samples (of batch size <span  class="math">\(N\)</span>) for the generated image, but the parameters are modulated by <span  class="math">\(\gamma\)</span> and $\beta$ which are learned parameters from the segmentation mask. In implementation, these values are learned through a simple 2-layer convolution network, which maps the segmentation mask to corresponding scaling and bias values at each <span  class="math">\(h\)</span> and <span  class="math">\(w\)</span> coordinate of the activation map.</p>

<p><figure><img src="/img/bestperformer.png" alt="Best performing model - NOT the best perceptually accurate results" title="Figure 3 from the original paper: Qualitative outputs of the top performing model (without content and style transfer through AdaIN and SPADE). The left three columns show the input reference image, pseudo label and target label. The three columns on the right show the learned residual map, the final predicted image and the target image. The modified regions appear blurry. Compare this with the results below."><figcaption>Figure 3 from the original paper: Qualitative outputs of the top performing model (without content and style transfer through AdaIN and SPADE). The left three columns show the input reference image, pseudo label and target label. The three columns on the right show the learned residual map, the final predicted image and the target image. The modified regions appear blurry. Compare this with the results below.</figcaption></figure></p>

<p>Through the use of AdaIN and Gram matrix-derived losses to control style, and SPADE and feature map-derived losses to control segmentation content, the authors applied Seg2Eye in the OpenEDS Challenge <sup class="footnote-ref" id="fnref:Garbin2019OpenEDS-Dataset"><a class="footnote" href="#fn:Garbin2019OpenEDS-Dataset">11</a></sup> to generate synthetic images of eyes given a segmentation mask. The scores were computed as the L2 distance between the synthetic (<span  class="math">\(\hat{I}\)</span>) and ground truth (<span  class="math">\(I\)</span>) images associated with each mask as <span  class="math">\(\frac{1}{H W} \sqrt{\Sigma_{i}^{H} \Sigma_{j}^{W}\left(\hat{I}_{i j}-I_{i j}\right)^{2}}\)</span>, averaged across test samples from 152 participants. While the Seg2Eye model does not match the performance level of the most accurate model, the qualitative or perceptual results (Fig. 4 in <sup class="footnote-ref" id="fnref:Buhler2019Content-ConsistentStyle"><a class="footnote" href="#fn:Buhler2019Content-ConsistentStyle">12</a></sup>) appear much closer to the ground truth relative to the top performer (Fig. 3 in <sup class="footnote-ref" id="fnref:Buhler2019Content-ConsistentStyle"><a class="footnote" href="#fn:Buhler2019Content-ConsistentStyle">13</a></sup>). The authors do not report the precise mean accuracy, merely that it is less than the top-performing model (<span  class="math">\(<25.23\)</span>).</p>

<p><figure><img src="/img/seg2eye.png" alt="Best perceptually accurate results" title="Figure 4 from the original paper: Qualitative outputs of Seg2Eye. From left to right are (1) one of the style image inputs, (2) target segmentation mask input, (3) generated image, and (4) target real image taken from the validation set. The generated image closely follows the segmentation mask and input style."><figcaption>Figure 4 from the original paper: Qualitative outputs of Seg2Eye. From left to right are (1) one of the style image inputs, (2) target segmentation mask input, (3) generated image, and (4) target real image taken from the validation set. The generated image closely follows the segmentation mask and input style.</figcaption></figure></p>

<h1 id="strengths-and-limitations">Strengths and Limitations</h1>

<p>One of the primary reasons why researchers often use first-order fixation measures or limited pupil response around fixations is most likely due to noise in reliably capturing eye measures. Buhler et al's computational approach appears to outperform most submissions for the OpenEDS Challenge in attempts to address this problem. The authors work has large implications to other biomedical areas of research which require high quality image data. Through modeling an individual's external eye through a segmentation map and style image references, this area of work can vastly increase ease of data collection. Provided a small reference sample, simulated viewing behavior data may be able to strengthen predictive power for attention classification tasks. However, the specificity of the authors work is hard to judge, as their image generation techniques have not been tested rigorously outside of the OpenEDS dataset. Furthermore, the authors' Seg2Eye approach is weakened by the fact that it was not the top performer, despite producing relatively better perceptually similar images to the ground truth. This indicates a mismatch between the competition assessment technique, which compares pixel-by-pixel similarity, and qualitative assessment, or human judgement, of how close the artificial images are to human eye images.</p>
<div class="footnotes">

<hr>

<ol>
<li id="fn:vanRenswoude2018Gazepath-Quality">van Renswoude, D. R., Raijmakers, M. E. J., Koornneef, A., Johnson, S. P., Hunnius, S., &amp; Visser, I. (2018). Gazepath: An eye-tracking analysis tool that accounts for individual differences and data quality. Behavior Research Methods, 50(2), 834–852. <a href="https://doi.org/10.3758/s13428-017-0909-3">https://doi.org/10.3758/s13428-017-0909-3</a>
 <a class="footnote-return" href="#fnref:vanRenswoude2018Gazepath-Quality"><sup>[return]</sup></a></li>
<li id="fn:Blignaut2014Eye-trackingDesign">Blignaut, P., &amp; Wium, D. (2014). Eye-tracking data quality as affected by ethnicity and experimental design. Behavior Research Methods, 46(1), 67–80. <a href="https://doi.org/10.3758/s13428-013-0343-0">https://doi.org/10.3758/s13428-013-0343-0</a>
 <a class="footnote-return" href="#fnref:Blignaut2014Eye-trackingDesign"><sup>[return]</sup></a></li>
<li id="fn:buhleretal">Buhler, M., Park, S., De Mello, S., Zhang, X., &amp; Hilliges, O. (2019). Content-Consistent Generation of Realistic Eyes with Style. Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019, 2019-Janua, 4650–4654. <a href="https://doi.org/10.1109/ICCVW48693.2019.9130178">https://doi.org/10.1109/ICCVW48693.2019.9130178</a>
 <a class="footnote-return" href="#fnref:buhleretal"><sup>[return]</sup></a></li>
<li id="fn:Corvera1973TheTest">Corvera, J., Torres-Courtney, G., &amp; Lopez-Rios, G. (1973). The Neurotological Significance of Alterations of Pursuit Eye Movements and the Pendular Eye Tracking Test. Annals of Otology, Rhinology &amp; Laryngology, 82(6), 855–867. <a href="https://doi.org/10.1177/000348947308200620">https://doi.org/10.1177/000348947308200620</a>
 <a class="footnote-return" href="#fnref:Corvera1973TheTest"><sup>[return]</sup></a></li>
<li id="fn:Holzman1973Eye-TrackingSchizophrenia">Holzman, P. S., Proctor, L. R., &amp; Hughes, D. W. (1973). Eye-Tracking Patterns in Schizophrenia. (July), 179–182.
 <a class="footnote-return" href="#fnref:Holzman1973Eye-TrackingSchizophrenia"><sup>[return]</sup></a></li>
<li id="fn:Guillon2014VisualStudies">Guillon, Q., Hadjikhani, N., Baduel, S., &amp; Rogé, B. (2014). Visual social attention in autism spectrum disorder: Insights from eye tracking studies. Neuroscience and Biobehavioral Reviews, 42, 279–297. <a href="https://doi.org/10.1016/j.neubiorev.2014.03.013">https://doi.org/10.1016/j.neubiorev.2014.03.013</a>
 <a class="footnote-return" href="#fnref:Guillon2014VisualStudies"><sup>[return]</sup></a></li>
<li id="fn:Gatys2016AStyle">Gatys, L., Ecker, A., &amp; Bethge, M. (2016). A Neural Algorithm of Artistic Style. Journal of Vision, 16(12), 326. <a href="https://doi.org/10.1167/16.12.326">https://doi.org/10.1167/16.12.326</a>
 <a class="footnote-return" href="#fnref:Gatys2016AStyle"><sup>[return]</sup></a></li>
<li id="fn:Santurkar2019HowTraining">Guillon, Q., Hadjikhani, N., Baduel, S., &amp; Rogé, B. (2014). Visual social attention in autism spectrum disorder: Insights from eye tracking studies. Neuroscience and Biobehavioral Reviews, 42, 279–297. <a href="https://doi.org/10.1016/j.neubiorev.2014.03.013">https://doi.org/10.1016/j.neubiorev.2014.03.013</a>
 <a class="footnote-return" href="#fnref:Santurkar2019HowTraining"><sup>[return]</sup></a></li>
<li id="fn:Buhler2019Content-ConsistentStyle">Buhler, M., Park, S., De Mello, S., Zhang, X., &amp; Hilliges, O. (2019). Content-Consistent Generation of Realistic Eyes with Style. Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019, 2019-Janua, 4650–4654. <a href="https://doi.org/10.1109/ICCVW48693.2019.9130178">https://doi.org/10.1109/ICCVW48693.2019.9130178</a>
 <a class="footnote-return" href="#fnref:Buhler2019Content-ConsistentStyle"><sup>[return]</sup></a></li>
<li id="fn:Gatys2016AStyle">Gatys, L., Ecker, A., &amp; Bethge, M. (2016). A Neural Algorithm of Artistic Style. Journal of Vision, 16(12), 326. <a href="https://doi.org/10.1167/16.12.326">https://doi.org/10.1167/16.12.326</a>
 <a class="footnote-return" href="#fnref:Gatys2016AStyle"><sup>[return]</sup></a></li>
<li id="fn:Garbin2019OpenEDS-Dataset">Garbin, S. J., Shen, Y., Schuetz, I., Cavin, R., Hughes, G., &amp; Talathi, S. S. (2019). OpenEDS: Open eye dataset. ArXiv.
 <a class="footnote-return" href="#fnref:Garbin2019OpenEDS-Dataset"><sup>[return]</sup></a></li>
<li id="fn:Buhler2019Content-ConsistentStyle">Buhler, M., Park, S., De Mello, S., Zhang, X., &amp; Hilliges, O. (2019). Content-Consistent Generation of Realistic Eyes with Style. Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019, 2019-Janua, 4650–4654. <a href="https://doi.org/10.1109/ICCVW48693.2019.9130178">https://doi.org/10.1109/ICCVW48693.2019.9130178</a>
 <a class="footnote-return" href="#fnref:Buhler2019Content-ConsistentStyle"><sup>[return]</sup></a></li>
<li id="fn:Buhler2019Content-ConsistentStyle">Buhler, M., Park, S., De Mello, S., Zhang, X., &amp; Hilliges, O. (2019). Content-Consistent Generation of Realistic Eyes with Style. Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019, 2019-Janua, 4650–4654. <a href="https://doi.org/10.1109/ICCVW48693.2019.9130178">https://doi.org/10.1109/ICCVW48693.2019.9130178</a>
 <a class="footnote-return" href="#fnref:Buhler2019Content-ConsistentStyle"><sup>[return]</sup></a></li>
</ol>
</div>
    </div>

    
    


    
    

    <footer class="post-footer">
      <div class="post-tags">
          <a href="http://schko.github.io/tags/eye-tracking/">eye tracking</a>
          <a href="http://schko.github.io/tags/gan/">gan</a>
          
        </div>

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/google-aiy/">
            
            <i class="iconfont">
              <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

            </i>
            <span class="prev-text nav-default">speech to text and realistic speech synthesis</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/embeddings-fcn/">
            <span class="next-text nav-default">embeddings layers and dense connections</span>
            <span class="prev-text nav-mobile">Next</span>
            
            <i class="iconfont">
              <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

            </i>
          </a>
      </nav>
    </footer>
  </article>

  
  

  
  

  

  
  

  

  

  

    

  

        </div>
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="icon-links">
  
  
    <a href="mailto:munna@fractal.nyc" rel="me noopener" class="iconfont"
      title="email" >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="http://github.com/schko" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>


<a href="http://schko.github.io/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
   
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2019 -
    2021
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        Sharath Koorathota
        
      </span></span>

  
  

  
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont">
        
        <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

      </i>
    </div>
  </div>
  
<script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js" integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin="anonymous"></script>



  <script type="text/javascript">
    window.MathJax = {
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>



  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"
    integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG"
    crossorigin="anonymous">

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.js"
    integrity="sha384-JiKN5O8x9Hhs/UE5cT5AAJqieYlOZbGT3CHws/y97o3ty4R7/O5poG9F3JoiOYw1"
    crossorigin="anonymous"></script>

  
  <script defer
    src="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI"
    crossorigin="anonymous" onload="renderMathInElement(document.body);">
  </script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        
      });
    });
  </script>






  
    <script type="text/javascript" src="/js/load-photoswipe.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  















</body>
</html>
