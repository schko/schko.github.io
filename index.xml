<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning and Video</title>
    <link>http://schko.github.io/</link>
    <description>Recent content on Machine Learning and Video</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Sharath Koorathota</copyright>
    <lastBuildDate>Wed, 18 Mar 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://schko.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>about me</title>
      <link>http://schko.github.io/about/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/about/about/</guid>
      
        <description>
&lt;link rel=&#34;stylesheet&#34; href=&#34;http://schko.github.io/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34;&gt;
&lt;figure  class=&#34;right&#34;  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; 
  style=&#34;max-width:35%&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://schko.github.io/img/smk.JPG&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://schko.github.io/img/smk.JPG&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&#34;professional&#34;&gt;professional&lt;/h3&gt;
&lt;p&gt;creative director/researcher at &lt;a href=&#34;https://www.fractal.nyc&#34;&gt;fractal media&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;graduate researcher at &lt;a href=&#34;http://liinc.bme.columbia.edu/&#34;&gt;laboratory for intelligent imaging and neural computing&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;academic&#34;&gt;academic&lt;/h3&gt;
&lt;p&gt;ph.d. &lt;a href=&#34;http://bme.columbia.edu&#34;&gt;biomedical engineering&lt;/a&gt; at columbia university (in progress)&lt;/p&gt;
&lt;p&gt;m.s. &lt;a href=&#34;https://www.cs.columbia.edu/&#34;&gt;computer science&lt;/a&gt; at columbia university&lt;/p&gt;
&lt;p&gt;b.s. &lt;a href=&#34;http://www.sas.rochester.edu/bcs/&#34;&gt;brain and cognitive sciences&lt;/a&gt; at university of rochester&lt;/p&gt;
&lt;p&gt;b.a. &lt;a href=&#34;http://www.sas.rochester.edu/eco/index.html&#34;&gt;economics&lt;/a&gt; at university of rochester&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Contact:
&lt;a href=&#34;mailto:munna@fractal.nyc&#34;&gt;Email&lt;/a&gt; |
&lt;a href=&#34;https://github.com/schko&#34;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>scaling machine learning projects</title>
      <link>http://schko.github.io/post/google-cloud/</link>
      <pubDate>Wed, 18 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/post/google-cloud/</guid>
      
        <description>&lt;p&gt;A guide to scaling machine learning projects using Google&#39;s tools.&lt;/p&gt;
&lt;h1 id=&#34;the-problem&#34;&gt;the problem&lt;/h1&gt;
&lt;p&gt;As I&#39;m working on a project with eye tracking data, here are some issues I&#39;m facing that you may relate to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The dataset I want to look at is increasing in volume (e.g. sample-by-sample data vs. aggregated values),&lt;/li&gt;
&lt;li&gt;I want to run multiple notebook sessions without my laptop/lab machine dying or being disconnected from a tool like Google Colab,&lt;/li&gt;
&lt;li&gt;Model runs and debugging is taking a long time due to above points,&lt;/li&gt;
&lt;li&gt;I want to be able to experiement with state-of-the-art tools to allow me to delve deeper into my data (e.g. computer vision or NLP tools to allow interpretability from raw data)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you relate to any of the above, you may find this post useful.&lt;/p&gt;
&lt;h1 id=&#34;background-reading&#34;&gt;background reading&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jupyter-notebook.readthedocs.io/en/stable/notebook.html&#34;&gt;Jupyter Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/notebooks/welcome.ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developers.google.com/identity/protocols/oauth2&#34;&gt;Google API access and authentication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/ai-platform/notebooks/docs/create-new&#34;&gt;Google AI Platform notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/storage/docs/key-terms#buckets&#34;&gt;Google Storage buckets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;workflow-for-researchers&#34;&gt;workflow for researchers&lt;/h1&gt;
&lt;p&gt;After working on a few of these projects, these are the areas for me that are labor intensive and where scaling specifically can help. Deliberately leaving out production or consumer interactions of models of here to focus on development and research.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/workflow.png&#34; alt=&#34;Data workflow for researchers&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;how-to-read-the-workflow-diagram&#34;&gt;how to read the workflow diagram&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Traditional projects have four components:
&lt;ul&gt;
&lt;li&gt;processing: where the bulk of &lt;strong&gt;computational&lt;/strong&gt; time and effort is spent training models, debugging, etc.&lt;/li&gt;
&lt;li&gt;analysis: where the bulk of &lt;strong&gt;human&lt;/strong&gt; time and effort is spent designing protocols, deriving meaning from results, etc.&lt;/li&gt;
&lt;li&gt;storage: the &lt;strong&gt;infrastructure&lt;/strong&gt; that needs to allow quick (and cheap) access to data,&lt;/li&gt;
&lt;li&gt;tools: to keep everything moving and involve others.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Most research&lt;/em&gt; can be done using the tools colored in &lt;strong&gt;green&lt;/strong&gt;, since it involves data that is smaller, or tools that are well-defined,&lt;/li&gt;
&lt;li&gt;&lt;em&gt;One way&lt;/em&gt; scaling is done is through expanding to the &lt;strong&gt;red&lt;/strong&gt; blocks, where methods like multiprocessing, TPUs and aggregating industry tools in a complex way can potentially improve research,&lt;/li&gt;
&lt;li&gt;Data flows to and from storage and other tools interface with them to track progress and feed in new experimental ideas, indicated by &lt;strong&gt;arrows&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It&#39;s more complicated than this, since researchers now connect local/Colab-type systems to access cloud APIs, send and receive data from multiple storage sources, and use a whole host of other tools I probably haven&#39;t heard of. For me, generally, I think visualization summarizes the workflow.&lt;/p&gt;
&lt;h1 id=&#34;advantages-to-google-cloud&#34;&gt;advantages to google cloud&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Faster storage access: buckets are extremely efficient, and I found downloading a 2gb file from a bucket in Colab took over 4 minutes, whereas my default TF 2.0 AI Platform Notebook took 5 seconds. This is because of multiple CPUs available for cheap on Cloud,&lt;/li&gt;
&lt;li&gt;Stability: machines more rarely bug out on Cloud, since you can often allocate a virtual machine with slightly more resources than you will use,&lt;/li&gt;
&lt;li&gt;Access to Cloud APIs: yes, this is available locally and through Google Colab, but the Computer Vision or NLP APIs are just easier to use and work smoother within a Google Cloud VM,&lt;/li&gt;
&lt;li&gt;Scaling: you can more easily allocate CPUs, GPUs, TPUs, change server sites, or have end-to-end pipelines to scale existing data down the road within Cloud,&lt;/li&gt;
&lt;li&gt;More responsive: this comes with better allocation of resources and the host of multiprocessing tools built into accessing, processing and analyzing data.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;scaling&#34;&gt;scaling&lt;/h1&gt;
&lt;p&gt;The rest of this post will describe specific steps and references as I transition from higher-level tools designed for more broadly-applicable data workflows (&lt;strong&gt;green blocks&lt;/strong&gt;) to lower-level counterparts on Google Cloud (&lt;strong&gt;red blocks&lt;/strong&gt;). Please feel free to share your thoughts via email if you have suggestions!&lt;/p&gt;
&lt;h3 id=&#34;setup-google-bucket-and-transfer-data&#34;&gt;setup google bucket and transfer data&lt;/h3&gt;
&lt;p&gt;I would recommend going all in and setting up Google Storage buckets. You can access data from Google Drive from within AI Platform, Colab and even locally.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/storage/docs/creating-buckets&#34;&gt;Create a storage bucket&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@philipplies/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041&#34;&gt;Transfer your files form Google Drive to a bucket&lt;/a&gt;. The provided link provides a Colab notebook with easy to use commands,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/storage/docs/gsutil/commands/rsync&#34;&gt;Use rsync commands as needed to sync this folder&lt;/a&gt;. You can get fancy with this and have a script that periodically syncs a bucket with you Drive folder (if you&#39;re using Drive to collaborate).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; storage cost is pretty cheap, but may add up depending on how large your bucket is.&lt;/p&gt;
&lt;h3 id=&#34;ai-platform-notebook-setup&#34;&gt;ai platform notebook (setup)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/ai-platform/notebooks/docs/create-new#create_an_instance_with_default_properties&#34;&gt;Follow the Cloud tutorial to setup a notebook instance&lt;/a&gt;,&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You only start with a default engine for testing. Most likely you&#39;ll want the TF 2.0 version though if you are doing any model training.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; this creates a compute engine within Cloud. This is the main cost hog, unless you are adding GPUs or TPUs on top of this. When you start or stop an engine, it associates available resources to your instance, depending on what you requested. That&#39;s why the storage is &lt;strong&gt;persistent&lt;/strong&gt; but the computer resources are &lt;strong&gt;not&lt;/strong&gt; for an AI platform notebook.&lt;/p&gt;
&lt;h3 id=&#34;github-setup-for-analysis-notebooks&#34;&gt;github setup for analysis notebooks&lt;/h3&gt;
&lt;p&gt;I found it helpful to only leave notebooks within Github. All data should be downloaded from Google cloud, and analysis should be able to be done from scratching using the code in notebooks. This may be annoying, but I&#39;ve found it helpful to stick to it for replicability.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/getting-started-with-github/create-a-repo&#34;&gt;Create a new repo&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;You can simply go to File -&amp;gt; Clone a Repo to clone a private (prompts login) or public repo. If you need more help with this, &lt;a href=&#34;https://cloud.google.com/ai-platform/notebooks/docs/save-to-github&#34;&gt;see here&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Create your first notebook to import data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-transfer-protocol&#34;&gt;data transfer protocol&lt;/h3&gt;
&lt;p&gt;Data will first be imported into the Compute Engine that is associated with your AI Platform Notebook.&lt;/p&gt;
&lt;p&gt;I would recommend something like the following code block, with the following assumptions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;processing&lt;/strong&gt; is the folder where you store files downloaded from a bucket,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;import_folder&lt;/strong&gt; is the folder located in the bucket that will be used in the current notebook.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os
&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exists(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;processing&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;):
    os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;makedirs(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;processing&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
    
&lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; google.colab
  IN_COLAB &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; True
&lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt;:
  IN_COLAB &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; False
&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; IN_COLAB &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exists(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;processing&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;):
    &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; google.colab &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; auth
    auth&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;authenticate_user()

    &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;gcloud config set project {project_id}
    &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;gsutil &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;m cp &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;r gs:&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; {import_folder} processing&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; IN_COLAB &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exists(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;processing&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;importing..&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
    &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;gsutil &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;m cp &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;r gs:&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;{import_folder} processing&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;done!&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;processing folder exists. please check that this is what you want.&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This chunk will allow a different type of authentication depending on whether the notebook is run in Colab or Google Cloud. If you want to potentially collaborate with someone in Colab, publish your code for replicability or because you don&#39;t want to deal with permissions, you want the notebook to be generalizable. The &lt;strong&gt;IN_COLAB&lt;/strong&gt; variable can be used to trigger analysis and processing in the same notebook, depending on where it&#39;s run.&lt;/p&gt;
&lt;h4 id=&#34;transferring-data-back-to-a-bucket&#34;&gt;transferring data back to a bucket&lt;/h4&gt;
&lt;p&gt;It may be the case you want to transfer data back to a bucket. I would advise against this type of workflow, and would recommend keeping only raw data within a bucket. Any data generated as a result of processing should be replicable, so regeneration through a notebook into a Compute Engine is the way to go.&lt;/p&gt;
&lt;p&gt;However, it may be possible that you want to store a model H5 file or processed data. For this use the following gsutil command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gsutil cp processing/upload_image.png gs://{upload_folder}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;strong&gt;upload_folder&lt;/strong&gt; may be the same as your &lt;strong&gt;import_folder&lt;/strong&gt;. &lt;strong&gt;upload_image.png&lt;/strong&gt; is variable, and you may need to add the &lt;strong&gt;-r&lt;/strong&gt; option if you want to upload a folder to your bucket.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/storage/docs/quickstart-gsutil#create&#34;&gt;See here for more interactions with the bucket&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;analysis&lt;/h3&gt;
&lt;p&gt;Now that we have some data processed in the AI platform, you may be interested in analyzing the results. You can either do this in the same notebook within the AI Platform, or on Colab.&lt;/p&gt;
&lt;p&gt;I prefer to always have a link to Colab. The &lt;a href=&#34;https://colab.research.google.com/drive/1Xc8E8mKC4MBvQ6Sw6akd_X5Z1cmHSNca&#34;&gt;aforementioned sample Colab&lt;/a&gt; has code that will transfer files between buckets and local Drive locations. It&#39;s highly dependant on your use case what you will transfer, but the relevant portions you want are below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; google.colab &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; drive
drive&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mount(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/content/drive&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; google.colab &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; auth
auth&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;authenticate_user()

project_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;nifty-depth-246308&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;
&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;gcloud config set project {project_id}
&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;gsutil ls

bucket_name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;medium_demo_bucket_190710&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;

&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;gsutil &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;m cp &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;r gs:&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;{bucket_name}&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;analysis &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;content&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;drive&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;My\ Drive&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;Data&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this case, we connect Google Drive to Cloud, and copy files in the &lt;strong&gt;analysis&lt;/strong&gt; folder into a folder named &lt;strong&gt;data&lt;/strong&gt; on our Google Drive.&lt;/p&gt;
&lt;p&gt;The idea is that Colab is more replicable than working in Google Cloud, which is designed for faster processing and scability.&lt;/p&gt;
&lt;h1 id=&#34;lessons-learned&#34;&gt;lessons learned&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Monitor billing, and ask the support team if you&#39;re confused, they&#39;re pretty responsive,&lt;/li&gt;
&lt;li&gt;Try out free credit first, Cloud gives $300 for free.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;other-relevant-resources&#34;&gt;other relevant resources&lt;/h1&gt;
&lt;p&gt;-&lt;a href=&#34;https://github.com/GoogleCloudPlatform/ai-platform-samples&#34;&gt;See specific AI platform tutorials here&lt;/a&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>helpful (random) resources when learning eeg</title>
      <link>http://schko.github.io/post/eeg-resources/</link>
      <pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/post/eeg-resources/</guid>
      
        <description>&lt;p&gt;Part 3: Links and relevant literature relating to EEG in general and artifact correction.&lt;/p&gt;
&lt;h1 id=&#34;links&#34;&gt;links&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;The primary resource I found helpful is a &lt;a href=&#34;https://sccn.ucsd.edu/wiki/EEGLAB#The_EEGLAB_Tutorial_Outline&#34;&gt;tutorial from EEGLAB&lt;/a&gt;. Specifically, the single subject data processing which has a broad overview of processing EEG data.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;images&#34;&gt;images&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Headmap: &lt;a href=&#34;https://www.researchgate.net/figure/Head-map-This-figure-depicts-the-organization-of-the-EEG-electrode-sites_fig1_316856333&#34;&gt;https://www.researchgate.net/figure/Head-map-This-figure-depicts-the-organization-of-the-EEG-electrode-sites_fig1_316856333&lt;/a&gt;
&lt;img src=&#34;https://www.researchgate.net/profile/Eileen_Haebig/publication/316856333/figure/fig1/AS:505502740500480@1497532653203/Head-map-This-figure-depicts-the-organization-of-the-EEG-electrode-sites.png&#34; alt=&#34;Headmap&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Removing linear trends: &lt;a href=&#34;https://www.mathworks.com/help/matlab/data_analysis/detrending-data.html&#34;&gt;https://www.mathworks.com/help/matlab/data_analysis/detrending-data.html&lt;/a&gt;
&lt;img src=&#34;https://www.mathworks.com/help/examples/matlab/win64/RemovingLinearTrendsFromDataExample_02.png&#34; alt=&#34;removing trends&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data averaging methods and the visualizations they yeild: &lt;a href=&#34;https://sccn.ucsd.edu/wiki/Chapter_06:_Data_Averaging&#34;&gt;https://sccn.ucsd.edu/wiki/Chapter_06:_Data_Averaging&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;artifact-removal-and-correction&#34;&gt;artifact removal and correction&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Removing a mean &lt;strong&gt;baseline&lt;/strong&gt; value from each epoch is useful when baseline differences between data epochs (e.g., those arising from low frequency drifts or artifacts) are present. These are not meaningfully interpretable, but if left in the data could skew the data analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Removing &lt;strong&gt;linear trends&lt;/strong&gt; through high-pass filtering: &lt;a href=&#34;https://www.mathworks.com/help/matlab/data_analysis/detrending-data.html&#34;&gt;https://www.mathworks.com/help/matlab/data_analysis/detrending-data.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ICA&lt;/strong&gt; can be used to decompose data from either average reference, common reference, or bipolar reference channels &amp;ndash; or from more than one of these types at once. However, plotting single scalp maps requires that all channels use either the same common reference or the same average reference.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;EEG &lt;strong&gt;reference choice&lt;/strong&gt; has different effect on outcomes: &lt;a href=&#34;https://sapienlabs.org/effect-of-eeg-reference-choice-on-outcomes/&#34;&gt;https://sapienlabs.org/effect-of-eeg-reference-choice-on-outcomes/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Single trial analyses&lt;/strong&gt;: &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3210509/&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3210509/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;other-pointers&#34;&gt;other pointers&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;EEGLAB wiki on electrode locations and how signal is reported:&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: Equating &amp;lsquo;channel locations&amp;rsquo; with (single) electrode locations only makes sense when all channels use the same &amp;lsquo;reference channel.&amp;rsquo; An EEG channel signal is always the difference&amp;rsquo; between voltages at two (or more) electrodes &amp;ndash; typically some electrode &amp;ldquo;referred to&amp;rdquo; a reference channel (or channel combination. Equating the signal &amp;lsquo;channel location&amp;rsquo; to the location of one of the contributing electrodes is quite imprecise, as the channel must be equally sensitive to potentials flowing to either of its two (or more) contributing scalp electrodes.&lt;/p&gt;
&lt;/blockquote&gt;</description>
      
    </item>
    
    <item>
      <title>eye tracking in multimodal data</title>
      <link>http://schko.github.io/post/eeg-fmri-pupil-2/</link>
      <pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/post/eeg-fmri-pupil-2/</guid>
      
        <description>&lt;p&gt;Part 2: Mainly on eye tracking and pupillometry data.&lt;/p&gt;
&lt;h1 id=&#34;intention&#34;&gt;intention&lt;/h1&gt;
&lt;p&gt;Controlling for eye-related artifacts in EEG is a standard protocol, so I wanted to compile a list of the sources of electrical activity that result in distortion of EEG signal.&lt;/p&gt;
&lt;h1 id=&#34;we-may-not-need-to-remove-artifacts&#34;&gt;we may not need to remove artifacts&lt;/h1&gt;
&lt;p&gt;I outlined in my last post a couple of examples using deep learning models to either &amp;ldquo;learn noise&amp;rdquo; in order to remove it, or utilize noise with the end goal of assisting classification. There are similarities in how researchers use the word &amp;ldquo;noise&amp;rdquo; to how words like &amp;ldquo;artifacts&amp;rdquo; or &amp;ldquo;distortions&amp;rdquo; are also used, in my opinion.&lt;/p&gt;
&lt;p&gt;Including distortions in EEG may be essential to some research. A quick example of this is &lt;a href=&#34;https://github.com/tevisgehr/EEG-Classification&#34;&gt;Tevis Gher&#39;s project mentioned in the last post&lt;/a&gt;, where correction of blinking and other eye tracking related events would remove useful information that could aid in classification.&lt;/p&gt;
&lt;p&gt;I just mean to note that aggressive filtering or thresholding when finding independent components can lead to cases where we remove brain activity that affects the system of interest (e.g. EEG) and another that produces &amp;ldquo;artifacts&amp;rdquo; (e.g. blinking) to the detriment of the goal of a study (e.g. decision-making in a visual task where activity in certain regions of the brain affect blinking rates). In the parenthesized example of blink rate, it may be important to not remove blink artifacts when studying attention in children with ADHD, since Caplan et al &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; showed it may be a critical discriminatory measure between ADHD and non-ADHD populations.&lt;/p&gt;
&lt;h1 id=&#34;eye-related-artifacts&#34;&gt;eye related artifacts&lt;/h1&gt;
&lt;p&gt;With that out of the way, I wanted to summarize the types of corrections that seem to be most common in EEG processing, related to eye tracking data. This is based on (and the quotes below derive from) a very nice, controlled study by Plöchl, Ossandón and König in 2012 &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;artifact&lt;/th&gt;
&lt;th&gt;what is it&lt;/th&gt;
&lt;th&gt;what causes it (quotes from Plöchl et al)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;corneo-retinal dipole movement&lt;/td&gt;
&lt;td&gt;large ocular movements&lt;/td&gt;
&lt;td&gt;&lt;code&gt;orientation change of the eyeball and thus of the corneo-retinal dipole produced between the negatively charged retina and the positively charged cornea&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;blinks (eyelid-induced)&lt;/td&gt;
&lt;td&gt;spontaneous or willful blinking&lt;/td&gt;
&lt;td&gt;&lt;code&gt;eyelid slides down over the cornea, which is positively charged with respect to the forehead. Thereby the lid acts like a “sliding electrode,” short-circuiting the cornea to the scalp and producing artifacts in the EEG signal&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;eyelid-saccades&lt;/td&gt;
&lt;td&gt;saccade-accompanying ballistic eyelid movements&lt;/td&gt;
&lt;td&gt;&lt;code&gt;occur in synchrony with the rotation of the eyeball and therefore are not distinguishable from the corneo-retinal dipole offset in the raw data. During upward saccades, for instance, eyelid and eyeball move upwards with approximately the same speed. However, after the termination of both, eye- and eyelid saccades, the eyelid continues to slide more slowly for another 30–300 ms and produces a signal change that is observable particularly after upward saccades&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;saccadic spike potential&lt;/td&gt;
&lt;td&gt;potential appearing right before saccade onset&lt;/td&gt;
&lt;td&gt;depends on the type of eye movements. Characteristically, it&#39;s &lt;code&gt;a biphasic waveform, starting around 5ms prior to saccade onset and consisting ofa larger positive deflection followed by a smaller negative deflection&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;microsaccades&lt;/td&gt;
&lt;td&gt;involuntary, small eye movements that occur during fixations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;microsaccade-induced confounds in the EEG are mainly caused by spike potentials occurring at microsaccade onset, while orientation changes of the corneo-retinal dipole only play a minor role&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;correction-of-eye-artifacts&#34;&gt;correction of eye artifacts&lt;/h1&gt;
&lt;p&gt;This will be covered in the next post.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/0006322395003150&#34;&gt;https://www.sciencedirect.com/science/article/pii/0006322395003150&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/23087632&#34;&gt;https://www.ncbi.nlm.nih.gov/pubmed/23087632&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
      
    </item>
    
    <item>
      <title>eeg, fmri, &amp; eye tracking</title>
      <link>http://schko.github.io/post/eeg-fmri-pupil/</link>
      <pubDate>Tue, 21 Jan 2020 14:49:07 -0500</pubDate>
      
      <guid>http://schko.github.io/post/eeg-fmri-pupil/</guid>
      
        <description>&lt;p&gt;There is growing interest in multimodal, physiological data collection for the purposes of understanding decision-making, cognitive performance and designing brain computer interfaces.&lt;/p&gt;
&lt;p&gt;Part 1: Mainly on EEG artifacts&lt;/p&gt;
&lt;h1 id=&#34;summary-of-data-sources&#34;&gt;summary of data sources&lt;/h1&gt;
&lt;p&gt;The source of EEG activity is post-synaptic activity of cortical pyramidal cells, and glial cell activity measured at the scalp. fMRI is derived from the measurement of blood oxygen changes in different regions of the brain. Eye tracking is the primary tool for measuring pupillometry and gaze.&lt;/p&gt;
&lt;h1 id=&#34;where-each-excel&#34;&gt;where each excel&lt;/h1&gt;
&lt;p&gt;Eye tracking excels in paradigms where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;knowing if stimuli was seen,&lt;/li&gt;
&lt;li&gt;modeling decision-making (i.e. &lt;strong&gt;behavior before events of interest&lt;/strong&gt;) is a priority,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;EEG excels where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;temporal resolution of brain activity is important,&lt;/li&gt;
&lt;li&gt;measuring &lt;strong&gt;reactivity to stimuli or events&lt;/strong&gt; of interest is necessary,&lt;/li&gt;
&lt;li&gt;interacting with an interface using brain activity.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;fMRI excels where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;spatial resolution &lt;strong&gt;in the context of an task&lt;/strong&gt; is important,&lt;/li&gt;
&lt;li&gt;connectivity of brain regions and how it plays a role in decision-making is important.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;artifact-removal-as-a-function-of-adding-on-data-sources&#34;&gt;artifact removal (as a function of adding on data sources)&lt;/h1&gt;
&lt;p&gt;EEG appears to be the type of data that is, not only most prone to signal variations by itself, most effected from adding on other sources. For example, gradient artifacts that result from the fMRI scanner&#39;s magnetic field overwhelm the raw EEG signals in amplitude to the point where you cannot use the same EEG pipelines as you would in unimodal data collection. Abreu et al&#39;s review is a great resource for a summary of recent (but notably, non-deep learning) methods in removing the effects of gradient artifacts from EEG &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/fig2_abreau.png&#34; alt=&#34;A visualization of how EEG can be bastardized with fMRI input, without artifact removal and filtering.&#34;&gt;&lt;/p&gt;
&lt;p&gt;I couldn&#39;t help but feel lost in the signal processing background literature, but it was interesting to take a step back to read a study where artifacts and noise filtering in EEG, for the most part, wasn&#39;t really needed. It was a part of a &lt;a href=&#34;https://github.com/tevisgehr/EEG-Classification&#34;&gt;poster presentation by Tevis Gher&lt;/a&gt; at UNL, and it highlights the potential in using deep learning techniques to avoid tedious signal processing pipelines, &lt;strong&gt;but only for classification problems&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The task is a discriminatory one, where a system classifies whether an EEG waveform derived from an individual who is visualizing a familiar vs. unfamiliar motor-based task. In this case, it&#39;s not really important to filter noise, artifacts, etc. A convolutional system is capable of ignoring variations in the signal that are not relevant to the task at hand. In some cases, the results used in the last stage (2D visualizations below) probably, meaningfully, use artifacts that may be problematic in other contexts. For example, filtering blinking may be preferred in most EEG contexts, but it may not be an &amp;ldquo;artifact&amp;rdquo; in a classification task such as this, if participants tend to blink more when imagining themselves doing a familiar task.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/eeg_architecture.png&#34; alt=&#34;A classification network used by Gher.&#34;&gt;&lt;/p&gt;
&lt;p&gt;In fact, &lt;a href=&#34;https://www.vision-systems.com/boards-software/article/14039375/fundamental-applications-of-deep-learning-networks&#34;&gt;image denoising has a practical use&lt;/a&gt; in computer vision, but (from an initial look) I did not see too much use of deep learning techniques for this purpose in cognition.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnhum.2018.00029/full&#34;&gt;https://www.frontiersin.org/articles/10.3389/fnhum.2018.00029/full&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
      
    </item>
    
    <item>
      <title>extending a python application using django</title>
      <link>http://schko.github.io/post/web-dev-django/</link>
      <pubDate>Mon, 20 Jan 2020 12:53:20 -0500</pubDate>
      
      <guid>http://schko.github.io/post/web-dev-django/</guid>
      
        <description>&lt;p&gt;We recently demoed an &lt;a href=&#34;http://fractal.nyc/fovea&#34;&gt;eye tracking concept&lt;/a&gt; that uses gaze and pupil patterns to modify video in real time, but are in the process of making it work on any browser.&lt;/p&gt;
&lt;p&gt;This comes with challenges, including having communication channels between frontend (i.e. browser) and backend (i.e. Python). &lt;a href=&#34;https://www.djangoproject.com/&#34;&gt;Django&lt;/a&gt; makes this process smooth, but comes with a steep learning curve if you are not familiar with software development.&lt;/p&gt;
&lt;h1 id=&#34;a-helpful-tutorial&#34;&gt;a helpful tutorial&lt;/h1&gt;
&lt;p&gt;If you face a similar challenge: &lt;strong&gt;expanding a Python application to work with a web interface,&lt;/strong&gt; then I recommend you start with the &lt;a href=&#34;https://channels.readthedocs.io/en/latest/tutorial/index.html&#34;&gt;Django chat server application&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;CodingEntrepreneur&#39;s &lt;a href=&#34;https://www.youtube.com/watch?v=RVH05S1qab8&#34;&gt;WebSockets walk-through&lt;/a&gt; was especially useful if you want to understand how data is transferred throughout the Django framework.&lt;/p&gt;
&lt;p&gt;Some aspects we (so far) found most useful in this tutorial are:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;concept&lt;/th&gt;
&lt;th&gt;definition&lt;/th&gt;
&lt;th&gt;what it&#39;s used for in the tutorial&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;web server&lt;/td&gt;
&lt;td&gt;&lt;code&gt;software or hardware dedicated to running satisfying World Wide Web client requests&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;storage of web pages associated with running a chat room&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;caching&lt;/td&gt;
&lt;td&gt;&lt;code&gt;software component that stores data so that future requests for that data&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;speeding up accessing images or chats already seen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;relational database&lt;/td&gt;
&lt;td&gt;&lt;code&gt;set of formally described tables from which data can be accessed or reassembled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;storage of chats that can only be accessed by each user&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;object-oriented python backend&lt;/td&gt;
&lt;td&gt;&lt;code&gt;object&#39;s state changed only by it&#39;s behaviors, methods that actually validate and change the state&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;easily readable and modifiable models that work with html and js frontend&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;websockets&lt;/td&gt;
&lt;td&gt;&lt;code&gt;computer communications protocol, providing full-duplex communication channels over a single TCP connection&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;channels that allow transfer of data and commands between the front and backends&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;diagram-of-the-data-transfer&#34;&gt;diagram of the data transfer&lt;/h1&gt;
&lt;p&gt;Below is a very rough diagram of how all the parts fit together for the chat application, but which applies to a large number of problems.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/chat_app_summary.jpg&#34; alt=&#34;Chat app Django framework&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-interesting-parts-from-the-diagram-above&#34;&gt;the interesting parts (from the diagram above)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The viewer &lt;strong&gt;views&lt;/strong&gt; file receives and adds to the &lt;strong&gt;Frontend&lt;/strong&gt;, e.g. viewing a chat screen and submitting a message,&lt;/li&gt;
&lt;li&gt;The Python &lt;strong&gt;views.py&lt;/strong&gt; file contributes to the HTML by loading a template, filling a context and returning an HttpResponse object that modifies the html,&lt;/li&gt;
&lt;li&gt;The HTML initializes and opens a websocket and defines its behavior (i.e. when it executes),&lt;/li&gt;
&lt;li&gt;Python communicates with the socket through the &lt;strong&gt;consumers.py&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;The user does something (e.g. submits a message) on the interface that is received by the consumers &lt;strong&gt;receive&lt;/strong&gt; function,&lt;/li&gt;
&lt;li&gt;The receive function does something (e.g. modifies the message), and sends back a message using the &lt;strong&gt;receive&lt;/strong&gt; message,&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;onmessage&lt;/strong&gt; modifies the HTML in some way (e.g. posts the new message into a form that is viewable by the viewer).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this way, we get communication between a viewer, website, server, and storage (usually accessed and defined through a relational database in the &lt;strong&gt;models&lt;/strong&gt; file).&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>hugo resources</title>
      <link>http://schko.github.io/post/resources/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/post/resources/</guid>
      
        <description>&lt;p&gt;Some basic Hugo resources that I keep coming back to.&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/xianmin/hugo-theme-jane&#34;&gt;Jane&lt;/a&gt; theme for this webpage.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.xianmin.org/hugo-theme-jane/post/jane-theme-preview/&#34;&gt;Markdown shortcuts&lt;/a&gt; for the Jane theme.&lt;/li&gt;
&lt;li&gt;Another great &lt;a href=&#34;https://sourceforge.net/p/hugo-generator/wiki/markdown_syntax&#34;&gt;Markdown shortcuts&lt;/a&gt; resource&lt;/li&gt;
&lt;li&gt;Summary of the &lt;a href=&#34;https://www.jakewiesler.com/blog/hugo-directory-structure/&#34;&gt;Hugo file structure&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</description>
      
    </item>
    
  </channel>
</rss>
