<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning and Video</title>
    <link>http://schko.github.io/</link>
    <description>Recent content on Machine Learning and Video</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Sharath Koorathota</copyright>
    <lastBuildDate>Thu, 23 Jan 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://schko.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>about me</title>
      <link>http://schko.github.io/about/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/about/about/</guid>
      
        <description>
&lt;link rel=&#34;stylesheet&#34; href=&#34;http://schko.github.io/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34;&gt;
&lt;figure  class=&#34;right&#34;  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; 
  style=&#34;max-width:35%&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://schko.github.io/img/smk.JPG&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://schko.github.io/img/smk.JPG&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&#34;professional&#34;&gt;professional&lt;/h3&gt;
&lt;p&gt;creative director/researcher at &lt;a href=&#34;https://www.fractal.nyc&#34;&gt;fractal media&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;graduate researcher at &lt;a href=&#34;http://liinc.bme.columbia.edu/&#34;&gt;laboratory for intelligent imaging and neural computing&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;academic&#34;&gt;academic&lt;/h3&gt;
&lt;p&gt;ph.d. &lt;a href=&#34;http://bme.columbia.edu&#34;&gt;biomedical engineering&lt;/a&gt; at columbia university (in progress)&lt;/p&gt;
&lt;p&gt;m.s. &lt;a href=&#34;https://www.cs.columbia.edu/&#34;&gt;computer science&lt;/a&gt; at columbia university&lt;/p&gt;
&lt;p&gt;b.s. &lt;a href=&#34;http://www.sas.rochester.edu/bcs/&#34;&gt;brain and cognitive sciences&lt;/a&gt; at university of rochester&lt;/p&gt;
&lt;p&gt;b.a. &lt;a href=&#34;http://www.sas.rochester.edu/eco/index.html&#34;&gt;economics&lt;/a&gt; at university of rochester&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Contact:
&lt;a href=&#34;mailto:munna@fractal.nyc&#34;&gt;Email&lt;/a&gt; |
&lt;a href=&#34;https://github.com/schko&#34;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>eye tracking in multimodal data</title>
      <link>http://schko.github.io/post/eeg-fmri-pupil-2/</link>
      <pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/post/eeg-fmri-pupil-2/</guid>
      
        <description>&lt;p&gt;Part 2: Mainly on eye tracking and pupillometry data.&lt;/p&gt;
&lt;h1 id=&#34;intention&#34;&gt;intention&lt;/h1&gt;
&lt;p&gt;Controlling for eye-related artifacts in EEG is a standard protocol, so I wanted to compile a list of the sources of electrical activity that result in distortion of EEG signal.&lt;/p&gt;
&lt;h1 id=&#34;we-may-not-need-to-remove-artifacts&#34;&gt;we may not need to remove artifacts&lt;/h1&gt;
&lt;p&gt;I outlined in my last post a couple of examples using deep learning models to either &amp;ldquo;learn noise&amp;rdquo; in order to remove it, or utilize noise with the end goal of assisting classification. There are similarities in how researchers use the word &amp;ldquo;noise&amp;rdquo; to how words like &amp;ldquo;artifacts&amp;rdquo; or &amp;ldquo;distortions&amp;rdquo; are also used, in my opinion.&lt;/p&gt;
&lt;p&gt;Including distortions in EEG may be essential to some research. A quick example of this is &lt;a href=&#34;https://github.com/tevisgehr/EEG-Classification&#34;&gt;Tevis Gher&#39;s project mentioned in the last post&lt;/a&gt;, where correction of blinking and other eye tracking related events would remove useful information that could aid in classification.&lt;/p&gt;
&lt;p&gt;I just mean to note that aggressive filtering or thresholding when finding independent components can lead to cases where we remove brain activity that affects the system of interest (e.g. EEG) and another that produces &amp;ldquo;artifacts&amp;rdquo; (e.g. blinking) to the detriment of the goal of a study (e.g. decision-making in a visual task where activity in certain regions of the brain affect blinking rates). In the parenthesized example of blink rate, it may be important to not remove blink artifacts when studying attention in children with ADHD, since Caplan et al &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; showed it may be a critical discriminatory measure between ADHD and non-ADHD populations.&lt;/p&gt;
&lt;h1 id=&#34;eye-related-artifacts&#34;&gt;eye related artifacts&lt;/h1&gt;
&lt;p&gt;With that out of the way, I wanted to summarize the types of corrections that seem to be most common in EEG processing, related to eye tracking data. This is based on (and the quotes below derive from) a very nice, controlled study by Plöchl, Ossandón and König in 2012 &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;artifact&lt;/th&gt;
&lt;th&gt;what is it&lt;/th&gt;
&lt;th&gt;what causes it (quotes from Plöchl et al)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;corneo-retinal dipole movement&lt;/td&gt;
&lt;td&gt;large ocular movements&lt;/td&gt;
&lt;td&gt;&lt;code&gt;orientation change of the eyeball and thus of the corneo-retinal dipole produced between the negatively charged retina and the positively charged cornea&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;blinks (eyelid-induced)&lt;/td&gt;
&lt;td&gt;spontaneous or willful blinking&lt;/td&gt;
&lt;td&gt;&lt;code&gt;eyelid slides down over the cornea, which is positively charged with respect to the forehead. Thereby the lid acts like a “sliding electrode,” short-circuiting the cornea to the scalp and producing artifacts in the EEG signal&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;eyelid-saccades&lt;/td&gt;
&lt;td&gt;saccade-accompanying ballistic eyelid movements&lt;/td&gt;
&lt;td&gt;&lt;code&gt;occur in synchrony with the rotation of the eyeball and therefore are not distinguishable from the corneo-retinal dipole offset in the raw data. During upward saccades, for instance, eyelid and eyeball move upwards with approximately the same speed. However, after the termination of both, eye- and eyelid saccades, the eyelid continues to slide more slowly for another 30–300 ms and produces a signal change that is observable particularly after upward saccades&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;saccadic spike potential&lt;/td&gt;
&lt;td&gt;potential appearing right before saccade onset&lt;/td&gt;
&lt;td&gt;depends on the type of eye movements. Characteristically, it&#39;s &lt;code&gt;a biphasic waveform, starting around 5ms prior to saccade onset and consisting ofa larger positive deflection followed by a smaller negative deflection&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;microsaccades&lt;/td&gt;
&lt;td&gt;involuntary, small eye movements that occur during fixations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;microsaccade-induced confounds in the EEG are mainly caused by spike potentials occurring at microsaccade onset, while orientation changes of the corneo-retinal dipole only play a minor role&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;correction-of-eye-artifacts&#34;&gt;correction of eye artifacts&lt;/h1&gt;
&lt;p&gt;This will be covered in the next post.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/0006322395003150&#34;&gt;https://www.sciencedirect.com/science/article/pii/0006322395003150&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/23087632&#34;&gt;https://www.ncbi.nlm.nih.gov/pubmed/23087632&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
      
    </item>
    
    <item>
      <title>eeg, fmri, &amp; eye tracking</title>
      <link>http://schko.github.io/post/eeg-fmri-pupil/</link>
      <pubDate>Tue, 21 Jan 2020 14:49:07 -0500</pubDate>
      
      <guid>http://schko.github.io/post/eeg-fmri-pupil/</guid>
      
        <description>&lt;p&gt;There is growing interest in multimodal, physiological data collection for the purposes of understanding decision-making, cognitive performance and designing brain computer interfaces.&lt;/p&gt;
&lt;p&gt;Part 1: Mainly on EEG artifacts&lt;/p&gt;
&lt;h1 id=&#34;summary-of-data-sources&#34;&gt;summary of data sources&lt;/h1&gt;
&lt;p&gt;The source of EEG activity is post-synaptic activity of cortical pyramidal cells, and glial cell activity measured at the scalp. fMRI is derived from the measurement of blood oxygen changes in different regions of the brain. Eye tracking is the primary tool for measuring pupillometry and gaze.&lt;/p&gt;
&lt;h1 id=&#34;where-each-excel&#34;&gt;where each excel&lt;/h1&gt;
&lt;p&gt;Eye tracking excels in paradigms where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;knowing if stimuli was seen,&lt;/li&gt;
&lt;li&gt;modeling decision-making (i.e. &lt;strong&gt;behavior before events of interest&lt;/strong&gt;) is a priority,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;EEG excels where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;temporal resolution of brain activity is important,&lt;/li&gt;
&lt;li&gt;measuring &lt;strong&gt;reactivity to stimuli or events&lt;/strong&gt; of interest is necessary,&lt;/li&gt;
&lt;li&gt;interacting with an interface using brain activity.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;fMRI excels where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;spatial resolution &lt;strong&gt;in the context of an task&lt;/strong&gt; is important,&lt;/li&gt;
&lt;li&gt;connectivity of brain regions and how it plays a role in decision-making is important.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;artifact-removal-as-a-function-of-adding-on-data-sources&#34;&gt;artifact removal (as a function of adding on data sources)&lt;/h1&gt;
&lt;p&gt;EEG appears to be the type of data that is, not only most prone to signal variations by itself, most effected from adding on other sources. For example, gradient artifacts that result from the fMRI scanner&#39;s magnetic field overwhelm the raw EEG signals in amplitude to the point where you cannot use the same EEG pipelines as you would in unimodal data collection. Abreu et al&#39;s review is a great resource for a summary of recent (but notably, non-deep learning) methods in removing the effects of gradient artifacts from EEG &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/fig2_abreau.png&#34; alt=&#34;A visualization of how EEG can be bastardized with fMRI input, without artifact removal and filtering.&#34;&gt;&lt;/p&gt;
&lt;p&gt;I couldn&#39;t help but feel lost in the signal processing background literature, but it was interesting to take a step back to read a study where artifacts and noise filtering in EEG, for the most part, wasn&#39;t really needed. It was a part of a &lt;a href=&#34;https://github.com/tevisgehr/EEG-Classification&#34;&gt;poster presentation by Tevis Gher&lt;/a&gt; at UNL, and it highlights the potential in using deep learning techniques to avoid tedious signal processing pipelines, &lt;strong&gt;but only for classification problems&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The task is a discriminatory one, where a system classifies whether an EEG waveform derived from an individual who is visualizing a familiar vs. unfamiliar motor-based task. In this case, it&#39;s not really important to filter noise, artifacts, etc. A convolutional system is capable of ignoring variations in the signal that are not relevant to the task at hand. In some cases, the results used in the last stage (2D visualizations below) probably, meaningfully, use artifacts that may be problematic in other contexts. For example, filtering blinking may be preferred in most EEG contexts, but it may not be an &amp;ldquo;artifact&amp;rdquo; in a classification task such as this, if participants tend to blink more when imagining themselves doing a familiar task.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/eeg_architecture.png&#34; alt=&#34;A classification network used by Gher.&#34;&gt;&lt;/p&gt;
&lt;p&gt;In fact, &lt;a href=&#34;https://www.vision-systems.com/boards-software/article/14039375/fundamental-applications-of-deep-learning-networks&#34;&gt;image denoising has a practical use&lt;/a&gt; in computer vision, but (from an initial look) I did not see too much use of deep learning techniques for this purpose in cognition.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnhum.2018.00029/full&#34;&gt;https://www.frontiersin.org/articles/10.3389/fnhum.2018.00029/full&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
      
    </item>
    
    <item>
      <title>extending a python application using django</title>
      <link>http://schko.github.io/post/web-dev-django/</link>
      <pubDate>Mon, 20 Jan 2020 12:53:20 -0500</pubDate>
      
      <guid>http://schko.github.io/post/web-dev-django/</guid>
      
        <description>&lt;p&gt;We recently demoed an &lt;a href=&#34;http://fractal.nyc/fovea&#34;&gt;eye tracking concept&lt;/a&gt; that uses gaze and pupil patterns to modify video in real time, but are in the process of making it work on any browser.&lt;/p&gt;
&lt;p&gt;This comes with challenges, including having communication channels between frontend (i.e. browser) and backend (i.e. Python). &lt;a href=&#34;https://www.djangoproject.com/&#34;&gt;Django&lt;/a&gt; makes this process smooth, but comes with a steep learning curve if you are not familiar with software development.&lt;/p&gt;
&lt;h1 id=&#34;a-helpful-tutorial&#34;&gt;a helpful tutorial&lt;/h1&gt;
&lt;p&gt;If you face a similar challenge: &lt;strong&gt;expanding a Python application to work with a web interface,&lt;/strong&gt; then I recommend you start with the &lt;a href=&#34;https://channels.readthedocs.io/en/latest/tutorial/index.html&#34;&gt;Django chat server application&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;CodingEntrepreneur&#39;s &lt;a href=&#34;https://www.youtube.com/watch?v=RVH05S1qab8&#34;&gt;WebSockets walk-through&lt;/a&gt; was especially useful if you want to understand how data is transferred throughout the Django framework.&lt;/p&gt;
&lt;p&gt;Some aspects we (so far) found most useful in this tutorial are:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;concept&lt;/th&gt;
&lt;th&gt;definition&lt;/th&gt;
&lt;th&gt;what it&#39;s used for in the tutorial&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;web server&lt;/td&gt;
&lt;td&gt;&lt;code&gt;software or hardware dedicated to running satisfying World Wide Web client requests&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;storage of web pages associated with running a chat room&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;caching&lt;/td&gt;
&lt;td&gt;&lt;code&gt;software component that stores data so that future requests for that data&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;speeding up accessing images or chats already seen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;relational database&lt;/td&gt;
&lt;td&gt;&lt;code&gt;set of formally described tables from which data can be accessed or reassembled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;storage of chats that can only be accessed by each user&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;object-oriented python backend&lt;/td&gt;
&lt;td&gt;&lt;code&gt;object&#39;s state changed only by it&#39;s behaviors, methods that actually validate and change the state&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;easily readable and modifiable models that work with html and js frontend&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;websockets&lt;/td&gt;
&lt;td&gt;&lt;code&gt;computer communications protocol, providing full-duplex communication channels over a single TCP connection&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;channels that allow transfer of data and commands between the front and backends&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;diagram-of-the-data-transfer&#34;&gt;diagram of the data transfer&lt;/h1&gt;
&lt;p&gt;Below is a very rough diagram of how all the parts fit together for the chat application, but which applies to a large number of problems.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/chat_app_summary.jpg&#34; alt=&#34;Chat app Django framework&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-interesting-parts-from-the-diagram-above&#34;&gt;the interesting parts (from the diagram above)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The viewer &lt;strong&gt;views&lt;/strong&gt; file receives and adds to the &lt;strong&gt;Frontend&lt;/strong&gt;, e.g. viewing a chat screen and submitting a message,&lt;/li&gt;
&lt;li&gt;The Python &lt;strong&gt;views.py&lt;/strong&gt; file contributes to the HTML by loading a template, filling a context and returning an HttpResponse object that modifies the html,&lt;/li&gt;
&lt;li&gt;The HTML initializes and opens a websocket and defines its behavior (i.e. when it executes),&lt;/li&gt;
&lt;li&gt;Python communicates with the socket through the &lt;strong&gt;consumers.py&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;The user does something (e.g. submits a message) on the interface that is received by the consumers &lt;strong&gt;receive&lt;/strong&gt; function,&lt;/li&gt;
&lt;li&gt;The receive function does something (e.g. modifies the message), and sends back a message using the &lt;strong&gt;receive&lt;/strong&gt; message,&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;onmessage&lt;/strong&gt; modifies the HTML in some way (e.g. posts the new message into a form that is viewable by the viewer).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this way, we get communication between a viewer, website, server, and storage (usually accessed and defined through a relational database in the &lt;strong&gt;models&lt;/strong&gt; file).&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>hugo resources</title>
      <link>http://schko.github.io/post/resources/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/post/resources/</guid>
      
        <description>&lt;p&gt;Some basic Hugo resources that I keep coming back to.&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/xianmin/hugo-theme-jane&#34;&gt;Jane&lt;/a&gt; theme for this webpage.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.xianmin.org/hugo-theme-jane/post/jane-theme-preview/&#34;&gt;Markdown shortcuts&lt;/a&gt; for the Jane theme.&lt;/li&gt;
&lt;li&gt;Another great &lt;a href=&#34;https://sourceforge.net/p/hugo-generator/wiki/markdown_syntax&#34;&gt;Markdown shortcuts&lt;/a&gt; resource&lt;/li&gt;
&lt;li&gt;Summary of the &lt;a href=&#34;https://www.jakewiesler.com/blog/hugo-directory-structure/&#34;&gt;Hugo file structure&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</description>
      
    </item>
    
  </channel>
</rss>
