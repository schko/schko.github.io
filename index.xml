<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning and Video</title>
    <link>http://schko.github.io/</link>
    <description>Recent content on Machine Learning and Video</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Sharath Koorathota</copyright>
    <lastBuildDate>Tue, 05 Jan 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://schko.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>about me</title>
      <link>http://schko.github.io/about/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/about/about/</guid>
      
        <description>
&lt;link rel=&#34;stylesheet&#34; href=&#34;http://schko.github.io/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34;&gt;
&lt;figure  class=&#34;right&#34;  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; 
  style=&#34;max-width:35%&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://schko.github.io/img/smk.JPG&#34; /&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://schko.github.io/img/smk.JPG&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&#34;professional&#34;&gt;professional&lt;/h3&gt;
&lt;p&gt;creative director/researcher at &lt;a href=&#34;https://www.fractal.nyc&#34;&gt;fractal media&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;graduate researcher at &lt;a href=&#34;http://liinc.bme.columbia.edu/&#34;&gt;laboratory for intelligent imaging and neural computing&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;academic&#34;&gt;academic&lt;/h3&gt;
&lt;p&gt;ph.d. &lt;a href=&#34;http://bme.columbia.edu&#34;&gt;biomedical engineering&lt;/a&gt; at columbia university (in progress)&lt;/p&gt;
&lt;p&gt;m.s. &lt;a href=&#34;https://www.cs.columbia.edu/&#34;&gt;computer science&lt;/a&gt; at columbia university&lt;/p&gt;
&lt;p&gt;b.s. &lt;a href=&#34;http://www.sas.rochester.edu/bcs/&#34;&gt;brain and cognitive sciences&lt;/a&gt; at university of rochester&lt;/p&gt;
&lt;p&gt;b.a. &lt;a href=&#34;http://www.sas.rochester.edu/eco/index.html&#34;&gt;economics&lt;/a&gt; at university of rochester&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Contact:
&lt;a href=&#34;mailto:munna@fractal.nyc&#34;&gt;Email&lt;/a&gt; |
&lt;a href=&#34;https://github.com/schko&#34;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>style and content transfer of eye images using gans</title>
      <link>http://schko.github.io/post/eye-gan/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/post/eye-gan/</guid>
      
        <description>&lt;p&gt;How discriminator losses and normalization can be used to transfer semantic content and style information. A summary of work by Buhler et al, 2019.&lt;/p&gt;

&lt;p&gt;The following is a snippet from my first doctoral exam.&lt;/p&gt;

&lt;h1 id=&#34;background&#34;&gt;Background&lt;/h1&gt;

&lt;p&gt;A particular challenge in gaze prediction is a lack of data to apply sophisticated models that may be better able to model eye behavior. In fact, lack of high quality eye tracking data is a problem in the field of HCI &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:vanRenswoude2018Gazepath-Quality&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:vanRenswoude2018Gazepath-Quality&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Blignaut2014Eye-trackingDesign&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:Blignaut2014Eye-trackingDesign&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, presented by individual variability, privacy concerns and noise in signals from sensors that may be more ubiquitious (e.g., laptop cameras) but imprecise compared to more reliable, in-lab setups (e.g., with IR sensors). Buhler et al&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:buhleretal&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:buhleretal&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; address this problem through an application of GANs capable of generating data that maintain both the semantic and style information contained in images of the eye. The authors aim to generate eye image data that preserves the semantic segmentation of eye features (i.e. pupil, iris, sclera) relevant to gaze prediction while also modeling participant-specific eye style, characterized by perceptual features such as skin quality around the eye or sclera vasculature.&lt;/p&gt;

&lt;p&gt;The main problem the authors address is the lack of 2D eye images, collected from infrared cameras sensors, for model training. Specifically, they sought to improve data volume for training gaze estimation models under occlusion in VR, synthesize person-specific eye images that satisfy a segmentation mask and follow the style of a specified person from only a few reference images. Furthermore, the authors propose a method to inject style and content information at scale, which has implications past HCI to clinical diagnoses using eye behavior markers in patients with scleritis, autism or schizophrenia &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Corvera1973TheTest&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:Corvera1973TheTest&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Holzman1973Eye-TrackingSchizophrenia&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:Holzman1973Eye-TrackingSchizophrenia&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Guillon2014VisualStudies&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:Guillon2014VisualStudies&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;The authors modify GANs, adversarial networks which typically take random noise as input. By introducing noise, the model can produce data from a variety of locations in the target distribution, subsequently transforming the noise into a meaningful output. In implementation, GANs utilize two loss functions: one that is designed to replicate a probability distribution through generation of new image instances, hence &amp;quot;generative,&amp;quot; and another that acts as a discriminator which outputs a probability of the new image instances being real. Because the authors work relies on extensive technical background, the focus of this brief will cover the design of discriminator loss functions to quantify segmentation and style difference between &amp;quot;fake&amp;quot; and &amp;quot;real&amp;quot; images and the use of normalization to generate convincing fake images. These are critical to understanding recent and future applications of GANs to clinical image data, specifically in the generation of artificial eye images through the authors&#39; proposed Seg2Eye network.&lt;/p&gt;

&lt;h2 id=&#34;preserving-eye-image-style-through-losses-and-normalization&#34;&gt;Preserving eye image style through losses and normalization&lt;/h2&gt;

&lt;p&gt;First we define the simple way in which style information difference is quantified across two images by the discriminator. The output of a kernel (i.e., filter) applied to a convolutional layer (&lt;span  class=&#34;math&#34;&gt;\(l\)&lt;/span&gt;) forms the feature map (&lt;span  class=&#34;math&#34;&gt;\(F^l\)&lt;/span&gt;) associated with the layer. The Gram matrix &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Gatys2016AStyle&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:Gatys2016AStyle&#34;&gt;7&lt;/a&gt;&lt;/sup&gt; is computed using data from specific layers and on a number of feature maps (&lt;span  class=&#34;math&#34;&gt;\(N_l\)&lt;/span&gt;). It is a quantification of the similarity between features in a layer, across all feature maps. The &lt;span  class=&#34;math&#34;&gt;\(N^l \times N^l\)&lt;/span&gt; Gram matrix is composed of elements indexed by maps &lt;span  class=&#34;math&#34;&gt;\(i, j\)&lt;/span&gt;:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\mathbf{G}^{l}=\left\langle\mathbf{F}_{i:}^{l}, \mathbf{F}_{j:}^{l}\right\rangle = \left[\begin{array}{c}
\mathbf{F}_{1:}^{T} \\
\mathbf{F}_{2:}^{l}{ }^{T} \\
\vdots \\
\mathbf{F}_{N_{l}:}^{l}
\end{array}\right]\left[\begin{array}{llll}
\mathbf{F}_{1:}^{l} &amp; \mathbf{F}_{2:}^{l} &amp; \cdots &amp; \mathbf{F}_{N_{l}}^{l}
\end{array}\right]
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(\mathbf{F}_{i:}\)&lt;/span&gt; represents the column representation of a feature map &lt;span  class=&#34;math&#34;&gt;\(i\)&lt;/span&gt;. The Gram matrix has been effectively used to quantify the amount of style loss of a generated image &lt;span  class=&#34;math&#34;&gt;\(\hat{I}\)&lt;/span&gt; with respect to a style target image &lt;span  class=&#34;math&#34;&gt;\(I\)&lt;/span&gt;. The style-specific loss &lt;span  class=&#34;math&#34;&gt;\(\mathcal{L}_{\text {Gram}}\)&lt;/span&gt; is calculated by Buhler et al as:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\mathcal{L}_{\text{Gram}}=\Sigma_{i=2}^{m}\left\|\mathbf{G}^{l}\left(F_{E}(\hat{I})\right)-\mathbf{G}^{l}\left(F_{E}(I)\right)\right\|_{1}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(E\)&lt;/span&gt; is a style encoder trained to discriminate between feature map activations of (real) style images and m is the number of feature maps in &lt;span  class=&#34;math&#34;&gt;\(E\)&lt;/span&gt;. The style loss metric computes the L1 distance (i.e. sum of absolute differences) between a generated and target image feature maps at the Seg2Eye discriminator layer &lt;span  class=&#34;math&#34;&gt;\(l\)&lt;/span&gt;. Thus, the style encoder is trained to distinguish between feature map activations of different styles of eye images, and this information is used in place of calculating the Gram matrix from feature maps within the Seg2Eye network downstream.&lt;/p&gt;

&lt;p&gt;Another key aspect of Seg2Eye is the use of adaptive instance normalization (AdaIN) by the generator, which replaces batch normalization typically used in deep networks for the purposes of style transfer. While batch normalization has arguably been one of the core advancements that has made deep learning feasible for a wide range of biomedical problems &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Santurkar2019HowTraining&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:Santurkar2019HowTraining&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;, normalizing batches of data loses style information. AdaIN uses style input to align the channel-wise mean and variance of a content input to match the style. In Seg2Eye, this is used after a convolutional network layer to normalize the generated image statistics using the style reference from a human&#39;s eye (Fig. 2a in &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Buhler2019Content-ConsistentStyle&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:Buhler2019Content-ConsistentStyle&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;), transferring qualities such as sclera perforations and skin around the eye to the generated image. For a given style reference image, AdaIN uses its layer activations &lt;span  class=&#34;math&#34;&gt;\(r\)&lt;/span&gt; and computes its mean &lt;span  class=&#34;math&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span  class=&#34;math&#34;&gt;\(\sigma\)&lt;/span&gt; from a layer:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://schko.github.io/img/buhler2a.png&#34; alt=&#34;Figure 2a from Buhler et al&#34; title=&#34;Figure 2a from the original paper: Seg2Eye architecture. Describes the authors&#39; novel SPADE+Style Block, which combines Adaptive Instance Normalization (AdaIN) and Spatially Adaptive Normalization (SPADE) to allow for simultaneous style and content injection into the generator at multiple scales.&#34;&gt;&lt;figcaption&gt;Figure 2a from the original paper: Seg2Eye architecture. Describes the authors&#39; novel SPADE+Style Block, which combines Adaptive Instance Normalization (AdaIN) and Spatially Adaptive Normalization (SPADE) to allow for simultaneous style and content injection into the generator at multiple scales.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\mu_{n c}(r)=\frac{1}{H W} \sum_{h=1}^{H} \sum_{w=1}^{W} r_{n c h w} \\
\sigma_{n c}(r)=\sqrt{\frac{1}{H W} \sum_{h=1}^{H} \sum_{w=1}^{W}\left(r_{n c h w}-\mu_{n c}(x)\right)^{2}+\epsilon}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(H, W\)&lt;/span&gt; represent the dimensions of the activations and &lt;span  class=&#34;math&#34;&gt;\(n, c\)&lt;/span&gt; represent the batch and channel (i.e. layer) respectively. Then, for a generated image, the AdaIN block uses the &lt;span  class=&#34;math&#34;&gt;\(\mu(r)\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\sigma(r)\)&lt;/span&gt; when scaling activations during learning:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\operatorname{AdaIN}(x, r)=\sigma(r)\left(\frac{x_{nc}-\mu(x)}{\sigma(x)}\right)+\mu(r)
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; refers to the normalized image activations associated with the generated image. AdaIN allows each sample to be normalized distinctly (unlike batch normalization), and furthermore utilizes first- and second-order statistics of preferred activations from the reference style image. Through shifting and scaling layer activations of the generated image via AdaIN and minimizing a loss function (&lt;span  class=&#34;math&#34;&gt;\(\mathcal{L}_{\text {Gram}}\)&lt;/span&gt;) which compares feature maps between the generated and reference image, the network maintains the content&#39;s spatial information but uses the human eye reference image to infer style information critical to preserving perceptual similarity of the eye. This was an important goal for the authors to improve data generation methods.&lt;/p&gt;

&lt;h2 id=&#34;preserving-eye-image-segmentations-through-losses-and-normalization&#34;&gt;Preserving eye image segmentations through losses and normalization&lt;/h2&gt;

&lt;p&gt;A notable aspect of Seg2Eye is its input: rather than random noise typical of GANs, it utilizes more contextual information in the form of a segmentation mask. In this case, the reference image of the map consisted of a pupil, sclera and iris segmentations. While the Gram matrix is useful for quantifying the difference in consistency of feature maps across images by the discriminator, simply taking the distance between feature maps allows us to learn broad content information contained in an image through a previously proposed &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Gatys2016AStyle&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:Gatys2016AStyle&#34;&gt;10&lt;/a&gt;&lt;/sup&gt; loss function:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\mathcal{L}_{\text{\mathcal{D}_F}}=\Sigma_{i=2}^{m}\left\|F_{D}^{l}(\hat{I})-F_{D}^{l}(I)\right\|_{1}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\( F_{D}^{l} \)&lt;/span&gt; represents the feature map at discriminator network &lt;span  class=&#34;math&#34;&gt;\(D\)&lt;/span&gt; layer &lt;span  class=&#34;math&#34;&gt;\(l\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Using a segmentation mask as input allows the generator to learn context, and requires a solution that achieves the opposite goal of AdaIN. Semantic segmentation treats multiple objects of the same class (e.g. eye segments) as a single entity (&amp;quot;pupil&amp;quot;). On the other hand, instance segmentation treats multiple objects of the same class (person-specific eye segmentation) as distinct individual objects (&amp;quot;pupil 1,&amp;quot; &amp;quot;pupil 2&amp;quot;). The authors use spatially adaptive denormalization (SPADE) to allow for content injection through modulating layer activations from a generated eye image using a segmentation map input &lt;span  class=&#34;math&#34;&gt;\(m\)&lt;/span&gt;. In effect, this translates to adjusting Eqs. 3 and 4 from AdaIN to makes channel-wise adjustments, separately for each layer:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\mu_{c}(x)=\frac{1}{N H W} \sum_{h=1}^{N} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{n c h w} \\
\sigma_{c}(x)=\sqrt{\frac{1}{N H W} \sum_{h=1}^{N} \sum_{h=1}^{H} \sum_{w=1}^{W}\left(x_{n c h w}-\mu_{c}(x)\right)^{2}+\epsilon}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\operatorname{SPADE}(x, m)=\gamma_{c h w}(m)\left(\frac{x_{nchw}-\mu(x)}{\sigma(x)}\right)+\beta_{c h w}(m)
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where &lt;span  class=&#34;math&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(\sigma\)&lt;/span&gt; are now computed across samples (of batch size &lt;span  class=&#34;math&#34;&gt;\(N\)&lt;/span&gt;) for the generated image, but the parameters are modulated by &lt;span  class=&#34;math&#34;&gt;\(\gamma\)&lt;/span&gt; and $\beta$ which are learned parameters from the segmentation mask. In implementation, these values are learned through a simple 2-layer convolution network, which maps the segmentation mask to corresponding scaling and bias values at each &lt;span  class=&#34;math&#34;&gt;\(h\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(w\)&lt;/span&gt; coordinate of the activation map.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://schko.github.io/img/bestperformer.png&#34; alt=&#34;Best performing model - NOT the best perceptually accurate results&#34; title=&#34;Figure 3 from the original paper: Qualitative outputs of the top performing model (without content and style transfer through AdaIN and SPADE). The left three columns show the input reference image, pseudo label and target label. The three columns on the right show the learned residual map, the final predicted image and the target image. The modified regions appear blurry. Compare this with the results below.&#34;&gt;&lt;figcaption&gt;Figure 3 from the original paper: Qualitative outputs of the top performing model (without content and style transfer through AdaIN and SPADE). The left three columns show the input reference image, pseudo label and target label. The three columns on the right show the learned residual map, the final predicted image and the target image. The modified regions appear blurry. Compare this with the results below.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Through the use of AdaIN and Gram matrix-derived losses to control style, and SPADE and feature map-derived losses to control segmentation content, the authors applied Seg2Eye in the OpenEDS Challenge &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Garbin2019OpenEDS-Dataset&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:Garbin2019OpenEDS-Dataset&#34;&gt;11&lt;/a&gt;&lt;/sup&gt; to generate synthetic images of eyes given a segmentation mask. The scores were computed as the L2 distance between the synthetic (&lt;span  class=&#34;math&#34;&gt;\(\hat{I}\)&lt;/span&gt;) and ground truth (&lt;span  class=&#34;math&#34;&gt;\(I\)&lt;/span&gt;) images associated with each mask as &lt;span  class=&#34;math&#34;&gt;\(\frac{1}{H W} \sqrt{\Sigma_{i}^{H} \Sigma_{j}^{W}\left(\hat{I}_{i j}-I_{i j}\right)^{2}}\)&lt;/span&gt;, averaged across test samples from 152 participants. While the Seg2Eye model does not match the performance level of the most accurate model, the qualitative or perceptual results (Fig. 4 in &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Buhler2019Content-ConsistentStyle&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:Buhler2019Content-ConsistentStyle&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;) appear much closer to the ground truth relative to the top performer (Fig. 3 in &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Buhler2019Content-ConsistentStyle&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:Buhler2019Content-ConsistentStyle&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;). The authors do not report the precise mean accuracy, merely that it is less than the top-performing model (&lt;span  class=&#34;math&#34;&gt;\(&lt;25.23\)&lt;/span&gt;).&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://schko.github.io/img/seg2eye.png&#34; alt=&#34;Best perceptually accurate results&#34; title=&#34;Figure 4 from the original paper: Qualitative outputs of Seg2Eye. From left to right are (1) one of the style image inputs, (2) target segmentation mask input, (3) generated image, and (4) target real image taken from the validation set. The generated image closely follows the segmentation mask and input style.&#34;&gt;&lt;figcaption&gt;Figure 4 from the original paper: Qualitative outputs of Seg2Eye. From left to right are (1) one of the style image inputs, (2) target segmentation mask input, (3) generated image, and (4) target real image taken from the validation set. The generated image closely follows the segmentation mask and input style.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;strengths-and-limitations&#34;&gt;Strengths and Limitations&lt;/h1&gt;

&lt;p&gt;One of the primary reasons why researchers often use first-order fixation measures or limited pupil response around fixations is most likely due to noise in reliably capturing eye measures. Buhler et al&#39;s computational approach appears to outperform most submissions for the OpenEDS Challenge in attempts to address this problem. The authors work has large implications to other biomedical areas of research which require high quality image data. Through modeling an individual&#39;s external eye through a segmentation map and style image references, this area of work can vastly increase ease of data collection. Provided a small reference sample, simulated viewing behavior data may be able to strengthen predictive power for attention classification tasks. However, the specificity of the authors work is hard to judge, as their image generation techniques have not been tested rigorously outside of the OpenEDS dataset. Furthermore, the authors&#39; Seg2Eye approach is weakened by the fact that it was not the top performer, despite producing relatively better perceptually similar images to the ground truth. This indicates a mismatch between the competition assessment technique, which compares pixel-by-pixel similarity, and qualitative assessment, or human judgement, of how close the artificial images are to human eye images.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:vanRenswoude2018Gazepath-Quality&#34;&gt;van Renswoude, D. R., Raijmakers, M. E. J., Koornneef, A., Johnson, S. P., Hunnius, S., &amp;amp; Visser, I. (2018). Gazepath: An eye-tracking analysis tool that accounts for individual differences and data quality. Behavior Research Methods, 50(2), 834–852. &lt;a href=&#34;https://doi.org/10.3758/s13428-017-0909-3&#34;&gt;https://doi.org/10.3758/s13428-017-0909-3&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:vanRenswoude2018Gazepath-Quality&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Blignaut2014Eye-trackingDesign&#34;&gt;Blignaut, P., &amp;amp; Wium, D. (2014). Eye-tracking data quality as affected by ethnicity and experimental design. Behavior Research Methods, 46(1), 67–80. &lt;a href=&#34;https://doi.org/10.3758/s13428-013-0343-0&#34;&gt;https://doi.org/10.3758/s13428-013-0343-0&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Blignaut2014Eye-trackingDesign&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:buhleretal&#34;&gt;Buhler, M., Park, S., De Mello, S., Zhang, X., &amp;amp; Hilliges, O. (2019). Content-Consistent Generation of Realistic Eyes with Style. Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019, 2019-Janua, 4650–4654. &lt;a href=&#34;https://doi.org/10.1109/ICCVW48693.2019.9130178&#34;&gt;https://doi.org/10.1109/ICCVW48693.2019.9130178&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:buhleretal&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Corvera1973TheTest&#34;&gt;Corvera, J., Torres-Courtney, G., &amp;amp; Lopez-Rios, G. (1973). The Neurotological Significance of Alterations of Pursuit Eye Movements and the Pendular Eye Tracking Test. Annals of Otology, Rhinology &amp;amp; Laryngology, 82(6), 855–867. &lt;a href=&#34;https://doi.org/10.1177/000348947308200620&#34;&gt;https://doi.org/10.1177/000348947308200620&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Corvera1973TheTest&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Holzman1973Eye-TrackingSchizophrenia&#34;&gt;Holzman, P. S., Proctor, L. R., &amp;amp; Hughes, D. W. (1973). Eye-Tracking Patterns in Schizophrenia. (July), 179–182.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Holzman1973Eye-TrackingSchizophrenia&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Guillon2014VisualStudies&#34;&gt;Guillon, Q., Hadjikhani, N., Baduel, S., &amp;amp; Rogé, B. (2014). Visual social attention in autism spectrum disorder: Insights from eye tracking studies. Neuroscience and Biobehavioral Reviews, 42, 279–297. &lt;a href=&#34;https://doi.org/10.1016/j.neubiorev.2014.03.013&#34;&gt;https://doi.org/10.1016/j.neubiorev.2014.03.013&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Guillon2014VisualStudies&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Gatys2016AStyle&#34;&gt;Gatys, L., Ecker, A., &amp;amp; Bethge, M. (2016). A Neural Algorithm of Artistic Style. Journal of Vision, 16(12), 326. &lt;a href=&#34;https://doi.org/10.1167/16.12.326&#34;&gt;https://doi.org/10.1167/16.12.326&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Gatys2016AStyle&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Santurkar2019HowTraining&#34;&gt;Guillon, Q., Hadjikhani, N., Baduel, S., &amp;amp; Rogé, B. (2014). Visual social attention in autism spectrum disorder: Insights from eye tracking studies. Neuroscience and Biobehavioral Reviews, 42, 279–297. &lt;a href=&#34;https://doi.org/10.1016/j.neubiorev.2014.03.013&#34;&gt;https://doi.org/10.1016/j.neubiorev.2014.03.013&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Santurkar2019HowTraining&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Buhler2019Content-ConsistentStyle&#34;&gt;Buhler, M., Park, S., De Mello, S., Zhang, X., &amp;amp; Hilliges, O. (2019). Content-Consistent Generation of Realistic Eyes with Style. Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019, 2019-Janua, 4650–4654. &lt;a href=&#34;https://doi.org/10.1109/ICCVW48693.2019.9130178&#34;&gt;https://doi.org/10.1109/ICCVW48693.2019.9130178&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Buhler2019Content-ConsistentStyle&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Gatys2016AStyle&#34;&gt;Gatys, L., Ecker, A., &amp;amp; Bethge, M. (2016). A Neural Algorithm of Artistic Style. Journal of Vision, 16(12), 326. &lt;a href=&#34;https://doi.org/10.1167/16.12.326&#34;&gt;https://doi.org/10.1167/16.12.326&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Gatys2016AStyle&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Garbin2019OpenEDS-Dataset&#34;&gt;Garbin, S. J., Shen, Y., Schuetz, I., Cavin, R., Hughes, G., &amp;amp; Talathi, S. S. (2019). OpenEDS: Open eye dataset. ArXiv.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Garbin2019OpenEDS-Dataset&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Buhler2019Content-ConsistentStyle&#34;&gt;Buhler, M., Park, S., De Mello, S., Zhang, X., &amp;amp; Hilliges, O. (2019). Content-Consistent Generation of Realistic Eyes with Style. Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019, 2019-Janua, 4650–4654. &lt;a href=&#34;https://doi.org/10.1109/ICCVW48693.2019.9130178&#34;&gt;https://doi.org/10.1109/ICCVW48693.2019.9130178&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Buhler2019Content-ConsistentStyle&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Buhler2019Content-ConsistentStyle&#34;&gt;Buhler, M., Park, S., De Mello, S., Zhang, X., &amp;amp; Hilliges, O. (2019). Content-Consistent Generation of Realistic Eyes with Style. Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019, 2019-Janua, 4650–4654. &lt;a href=&#34;https://doi.org/10.1109/ICCVW48693.2019.9130178&#34;&gt;https://doi.org/10.1109/ICCVW48693.2019.9130178&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Buhler2019Content-ConsistentStyle&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
      
    </item>
    
    <item>
      <title>embeddings layers and dense connections</title>
      <link>http://schko.github.io/post/embeddings-fcn/</link>
      <pubDate>Sat, 11 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/post/embeddings-fcn/</guid>
      
        <description>&lt;p&gt;On the relatedness between embeddings and dense connections.&lt;/p&gt;

&lt;h1 id=&#34;the-question&#34;&gt;the question&lt;/h1&gt;

&lt;p&gt;Embedding layers are extensively used in language learning and recently we &lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3379157.3391653?casa_token=ibtiNK3KW4sAAAAA%3AQNuOsVK-mCEZkfsurtsM9FF35O9CnXCrB73FOQixHTToXBUNhJbBGfIKM2QUYz0uRsdE2f_5u31kAQ&#34;&gt;shared results&lt;/a&gt; showing improvements in prediction when applying them to sequences of categorical data. There is a nagging question, though, on whether they&#39;re really different from dense networks where weights appear to be learned the same way, and whether we avoid any costly operations in most cases.&lt;/p&gt;

&lt;h1 id=&#34;why-embeddings&#34;&gt;why embeddings&lt;/h1&gt;

&lt;p&gt;The point of embeddings is simply finding quantitative representations of a large number of categories in their contribution to classification of prediction. For example, in the word representation problem where we want to know what word follows &lt;em&gt;ants&lt;/em&gt; in a sentence:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;input&lt;/strong&gt; is a one-hot vector representing the input word (e.g. ants),&lt;/li&gt;
&lt;li&gt;hidden layer of variable number of dense nodes (the, learned lookup table),&lt;/li&gt;
&lt;li&gt;a softmax layer that forces a probability distribution,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;output&lt;/strong&gt; is a single vector containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word (e.g. car).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is outlined in the figure below.
&lt;figure&gt;&lt;img src=&#34;http://schko.github.io/img/word-representations.png&#34; alt=&#34;Source: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;A &lt;a href=&#34;https://stackoverflow.com/questions/34870614/what-does-tf-nn-embedding-lookup-function-do&#34;&gt;response on StackOverflow by the user kmario23&lt;/a&gt; provides a good start to defining variables for this post, I&#39;ve added additional, Tensorflow-specific functions to make this practical. Embedding matrices are typically trained from large (e.g. language) corpora, and are of the shape &lt;strong&gt;vocab_size * embedding_dim&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Suppose the following vocabulary:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vocab : [&#39;the&#39;,&#39;like&#39;,&#39;between&#39;,&#39;did&#39;,&#39;just&#39;,&#39;national&#39;,
		&#39;day&#39;,&#39;country&#39;,&#39;under&#39;,&#39;such&#39;,&#39;second&#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Imagine the following, trained word2vec vectors:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\small{ the : \left( 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 \right)}\\
\small{ like : \left( 0.36808 0.20834 -0.22319 0.046283 0.20098 0.27515 -0.77127 -0.76804 \right)}\\
\small{ between : \left( 0.7503 0.71623 -0.27033 0.20059 -0.17008 0.68568 -0.061672 -0.054638 \right)}\\
\small{ did : \left( 0.042523 -0.21172 0.044739 -0.19248 0.26224 0.0043991 -0.88195 0.55184 \right)}\\
\small{ just : \left( 0.17698 0.065221 0.28548 -0.4243 0.7499 -0.14892 -0.66786 0.11788 \right)}\\
\small{ national : \left( -1.1105 0.94945 -0.17078 0.93037 -0.2477 -0.70633 -0.8649 -0.56118 \right)}\\
\small{ day : \left( 0.11626 0.53897 -0.39514 -0.26027 0.57706 -0.79198 -0.88374 0.30119 \right)}\\
\small{ country : \left( -0.13531 0.15485 -0.07309 0.034013 -0.054457 -0.20541 -0.60086 -0.22407 \right)}\\
\small{ under : \left( 0.13721 -0.295 -0.05916 -0.59235 0.02301 0.21884 -0.34254 -0.70213 \right)}\\
\small{ such : \left( 0.61012 0.33512 -0.53499 0.36139 -0.39866 0.70627 -0.18699 -0.77246 \right)}\\
\small{ second : \left( -0.29809 0.28069 0.087102 0.54455 0.70003 0.44778 -0.72565 0.62309 \right)}\\
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The embedding matrix is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;emb = np.array([[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862],
   [0.36808, 0.20834, -0.22319, 0.046283, 0.20098, 0.27515, -0.77127, -0.76804],
   [0.7503, 0.71623, -0.27033, 0.20059, -0.17008, 0.68568, -0.061672, -0.054638],
   [0.042523, -0.21172, 0.044739, -0.19248, 0.26224, 0.0043991, -0.88195, 0.55184],
   [0.17698, 0.065221, 0.28548, -0.4243, 0.7499, -0.14892, -0.66786, 0.11788],
   [-1.1105, 0.94945, -0.17078, 0.93037, -0.2477, -0.70633, -0.8649, -0.56118],
   [0.11626, 0.53897, -0.39514, -0.26027, 0.57706, -0.79198, -0.88374, 0.30119],
   [-0.13531, 0.15485, -0.07309, 0.034013, -0.054457, -0.20541, -0.60086, -0.22407],
   [ 0.13721, -0.295, -0.05916, -0.59235, 0.02301, 0.21884, -0.34254, -0.70213],
   [ 0.61012, 0.33512, -0.53499, 0.36139, -0.39866, 0.70627, -0.18699, -0.77246 ],
   [ -0.29809, 0.28069, 0.087102, 0.54455, 0.70003, 0.44778, -0.72565, 0.62309 ]])


emb.shape
# (11, 8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will come back to using &lt;code&gt;emb&lt;/code&gt; in exploring how dense layers and embedding layers are related.&lt;/p&gt;

&lt;h1 id=&#34;weights-in-vectorized-representations&#34;&gt;weights in vectorized representations&lt;/h1&gt;

&lt;p&gt;In the example above, and in general, embeddings are simply hidden layer weights. Whether it&#39;s the word2vec representations, or the more sophisticated, context-sensitive, representations like &lt;a href=&#34;https://www.aclweb.org/anthology/N18-1202/&#34;&gt;ELMo&lt;/a&gt;, the goal involves learning a linear combination of layer representations.&lt;/p&gt;

&lt;p&gt;Running with the ELMo comparison, the difference in the embeddings from the one derived from the single, hidden layer is in the way we combine hidden layer outputs. The motivation is to combine context-dependent aspects of a word meaning (using higher-level LSTM layer weights), and syntax from lower-level LSTM states in order to represent more information about a token in a single vector.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
 ELMo_{k}^{\operatorname{task}}=\gamma^{\operatorname{task}} \sum_{j=0}^{L} s_{j}^{\operatorname{task}} \mathbf{h}_{k, j}^{L M}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\( \gamma \)&lt;/span&gt; acts as a (task-specific) scale to the ELMo vector,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\( s \)&lt;/span&gt; are the softmax-normalized weights,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\( k \)&lt;/span&gt; is the index of the word of interest,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\( j \)&lt;/span&gt; is the index of the layers (total &lt;span  class=&#34;math&#34;&gt;\( L \)&lt;/span&gt;) the weighted sum is calculated over,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\( h_{k,j} \)&lt;/span&gt;  is the output of the &lt;span  class=&#34;math&#34;&gt;\( j \)&lt;/span&gt;-th LSTM for the word &lt;span  class=&#34;math&#34;&gt;\( k \)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;In summary, we train a multi-layer, bi-directional, LSTM from a large corpus, extract the hidden state of each layer for the input sequence of words, compute a weighted sum of those hidden states to arrive at our final vectorized representation, or embedding. The weight of each hidden state is task-dependent and is learned separately.&lt;/p&gt;

&lt;h1 id=&#34;using-pretrained-embeddings-vs-embedding-layers&#34;&gt;using pre-trained embeddings vs. embedding layers&lt;/h1&gt;

&lt;p&gt;Realated to the overall question of how and why different the results of a study would be using embeddings vs. a fully connected layer, &lt;a href=&#34;https://towardsdatascience.com/pre-trained-word-embeddings-or-embedding-layer-a-dilemma-8406959fd76c&#34;&gt;recent work&lt;/a&gt; that looked into using task-specific embedding layers or pre-trained embeddings trained on larger text corpora has found intersting results. For embeddings derived from a sufficiently large corpus, we can expect faster training times and lower final loss, from the original author, Meghdad Farahmand:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This can mean that for solving certain NLP tasks, when the training set at hand is sufficiently large (as was the case in the Sentiment Analysis experiments), it is better to use pre-trained word embeddings. But if they are not available, you can still use an embedding layer and expect comparable results. If, however, the training set is small, the above experiments strongly encourage the use of pre-trained word embeddings.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;embeddings-are-simplified-dense-layers&#34;&gt;embeddings are simplified, dense layers&lt;/h1&gt;

&lt;p&gt;In addressing the relation between embeddings and dense layers, I was in search of a clear, functional explanation to why embeddings may be preferred when we have words as input or sequenced, categorical variables over ouputs from dense layers. Coming back to our initialization from before, imagine we want to convert the following sentence to an embedding:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sentence_to_embed = &#39;like national day&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Attempting to use a naive, dense network will need the following process in order to convert this sentence to its vectorized representation:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[
\text{sentence_to_embed} \rightarrow \text{one hot encoding matrix} \rightarrow \\ \text{matrix multiplication with emb}
\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The Tensorflow process would go something like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;inputs = one_hot(sentence_to_embed)
print(inputs)
# array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
	[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
	[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]] dtype=float32)

outputs = matmul(inputs, emb)  # kernel matrix
print(outputs)
# array([[0.36808, 0.20834, -0.22319, 0.046283, 0.20098, 0.27515, -0.77127, -0.76804],
       [-1.1105, 0.94945, -0.17078, 0.93037, -0.2477, -0.70633, -0.8649, -0.56118],
       [0.11626, 0.53897, -0.39514, -0.26027, 0.57706, -0.79198, -0.88374, 0.30119]], dtype=float32)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There&#39;s a problem here: the &lt;code&gt;inputs&lt;/code&gt; size increases dramatically with language size. The embedding layer performs a shortcut: it looks up the index of the words in &lt;code&gt;sentence_to_embed&lt;/code&gt; and references the relevant rows of &lt;code&gt;emb&lt;/code&gt; to avoid the dot product:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;outputs = tf.gather(emb, sentence_to_embed)
# same output as above&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That&#39;s it for the difference. Embedding layers are limited dense layers, with two limitations: the inputs need to be integers, and you cannot use the bias or activation aspects of dense layers - only the kernel.&lt;/p&gt;

&lt;h1 id=&#34;takeaway&#34;&gt;takeaway&lt;/h1&gt;

&lt;p&gt;An embedding layer is a limited version of a dense layer. It is useful to speed up training and to yeild lower final loss, because it avoids costly matrix multiplication through the assumption that inputs can be referenced by their indices.&lt;/p&gt;

&lt;p&gt;Embeddings are best used in pairing with raw input, as in the case of ELMo where the embeddings can be concatenated with one-hot encodings for certain problems, where the vectorized representations of categorical data or words was pre-trained (&lt;a href=&#34;https://allennlp.org/elmo&#34;&gt;see here for some downloads&lt;/a&gt;). Furthermore, embeddings can be simple (dense layer weights) or more complicated (weighing multiple layer weights with some task-specific scaling).&lt;/p&gt;

&lt;p&gt;Realizing that it&#39;s is not feasible to train dense networks for your problem will strongly suggest that embeddings will improve model performance, but take a look at the Meghdad&#39;s application above to see if the performance improvements would be worth it.&lt;/p&gt;

&lt;h1 id=&#34;other-relevant-resources&#34;&gt;other relevant resources&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/&#34;&gt;Great summary of the progress and types of langauge embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/47868265/what-is-the-difference-between-an-embedding-layer-and-a-dense-layer&#34;&gt;Some helpful Stackoverflow responses on the topic&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
      
    </item>
    
    <item>
      <title>remote training with eye tracking</title>
      <link>http://schko.github.io/post/fovea-training/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/post/fovea-training/</guid>
      
        <description>&lt;p&gt;There&#39;s been some great progress with &lt;a href=&#34;http://www.foveainsights.com&#34;&gt;Fovea&lt;/a&gt;, a platform using remote, webcam-based eye tracking for verification and interaction.&lt;/p&gt;
&lt;h1 id=&#34;overview&#34;&gt;overview&lt;/h1&gt;
&lt;p&gt;Fovea is a web platform that enables interactive content delivery through analyzing viewing patterns. It&#39;s an initiative &lt;a href=&#34;https://www.patrickadelman.com/&#34;&gt;Patrick Adelman&lt;/a&gt; and I started earlier this year, and we recently published a series of blog posts with some major updates:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/fovea/an-intro-to-fovea-using-eye-tracking-for-interactive-content-delivery-e55ed26bd538&#34;&gt;Overview of the platform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/fovea/interactive-remote-training-using-eye-tracking-8a1a15973029&#34;&gt;Fovea&#39;s first application in improving the way employees are trained remotely&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/fovea/the-future-of-content-delivery-over-the-web-using-eye-tracking-565ae042c2b2&#34;&gt;The future of Fovea, our upcoming partnership with AP and in market research&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The following is a condensed version of our progress and application in the field of employee training.&lt;/p&gt;
&lt;h1 id=&#34;the-challenge&#34;&gt;the challenge&lt;/h1&gt;
&lt;p&gt;In the first stage, we&#39;re making learning more interactive. We&#39;re doing this through combining object-level data with real-time eye tracking.
&lt;img src=&#34;http://schko.github.io/img/fovea/how-it-works.jpeg&#34; alt=&#34;Fovea how it works&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Content is uploaded to the cloud by the user.&lt;/strong&gt; This could be video, training documents, graphics, photos, or any other form of media. State-of-the-art processing algorithms are used to analyze the content and prepare it for viewing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Viewers enable the standard webcam on their laptop&lt;/strong&gt; or stand in front of a display with a simple camera. No special software is needed, Fovea can run entirely through a web browser. If desired, Fovea can verify which user is currently watching — for cases of remote training or online education, this will be useful.&lt;/li&gt;
&lt;li&gt;While the viewer is viewing the content, &lt;strong&gt;Fovea tracks and processes their eye movements in real time&lt;/strong&gt;, in connection with the analysis from step 1. This allows Fovea to understand exactly what is drawing a viewer’s attention and serve content based on that.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Analytics are generated&lt;/strong&gt; for the user to determine which content was effective and how to better design viewer experiences going forward.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;training-pilot-study&#34;&gt;training pilot study&lt;/h1&gt;
&lt;p&gt;We selected a &lt;a href=&#34;https://www.youtube.com/watch?v=BRx_pCaMKI0&amp;amp;feature=youtu.be&#34;&gt;video on OSHA safety signs&lt;/a&gt; that are commonly encountered in manufacturing facilities, and specified areas of interest (AOIs) that a training manager may label as important.&lt;/p&gt;
&lt;p&gt;Next, we recruited a small number of participants online through MTurk, who viewed the training video and answered quizzes, triggered at various times. Each personalized quiz question was based on the AOI they viewed least. In other words, if important regions in the video weren’t attended to as much as they should have, participants were presented with a multiple-choice quiz that drew their attention to areas that were labeled as important.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/fovea/training.gif&#34; alt=&#34;Components of the training experience&#34;&gt;&lt;/p&gt;
&lt;p&gt;Eye tracking was done through the participants’ web camera, after detection of the eye region, shown at all times to the participants so they were aware of what data was being analyzed. We did not need to store any image data, and only used the frame-by-frame footage to extrapolate eye tracking coordinates client-side.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;results&lt;/h2&gt;
&lt;p&gt;We were initially worried about the problems posed by the variety of environmental effectors on the quality of eye tracking data, e.g. poor lighting, laptop setup. However, we found that our eye detector was able to detect the eye region with high confidence (&amp;gt; 85% of the time), and the majority of samples (&amp;gt;74%) detected that viewers were looking at the video region.&lt;/p&gt;
&lt;p&gt;Next, I was very curious about how well our dynamic quiz performance corresponded with looking time. We&#39;re not publishing these results anytime soon since it&#39;s just a feasibility study, but we were encouraged to see that we not only were able to present quiz questions based on viewed regions, in real time, but this actually related to how well participants performed on the assessments.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/fovea/attention-retention.png&#34; alt=&#34;Dynamic quiz performance vs. eye tracking&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;interactive-viewing&#34;&gt;interactive viewing&lt;/h2&gt;
&lt;p&gt;The novelty of Fovea relies on using eye tracking in real time to effect content delivery. In this application, the quiz questions were personalized to each viewer, and the question related to an AOI that they viewed &lt;strong&gt;least&lt;/strong&gt; in the preceding section of the video. In the image below, sections are labeled &lt;em&gt;&lt;strong&gt;Danger, Warning, Caution, Notice and General Safety&lt;/strong&gt;&lt;/em&gt;, and the selected sequence of quiz questions depended on AOIs that were viewed least (red border).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/fovea/quiz-sequence.png&#34; alt=&#34;Dynamic quiz performance vs. eye tracking&#34;&gt;&lt;/p&gt;
&lt;p&gt;Through presenting assessments dynamically, we guided viewers to paying attention to those AOIs that were quizzed on ~ 7% more of the time in the subsequent region. This type of interactivity is valuable in learning. You can read more &lt;a href=&#34;https://medium.com/fovea/the-future-of-content-delivery-over-the-web-using-eye-tracking-565ae042c2b2&#34;&gt;on our full post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;what&#39;s next?&lt;/h2&gt;
&lt;p&gt;Next, we&#39;re partnering with the Associated Press to present news content interactively. This is something I&#39;m very passionate about, every time I have to immediately close or mute a video on a news website since it never presents what I want to see. Inconvenience aside, interactive news content delivery can makes facts more engaging and personalized. Below is a preview of what you can expect: object-specific data is paired with real-time eye tracking to automate editing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/fovea/ap-delivery.gif&#34; alt=&#34;News content delivery through Fovea&#34;&gt;&lt;/p&gt;
&lt;p&gt;Read more about our future goals &lt;a href=&#34;https://medium.com/fovea/the-future-of-content-delivery-over-the-web-using-eye-tracking-565ae042c2b2&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>scaling machine learning projects</title>
      <link>http://schko.github.io/post/google-cloud/</link>
      <pubDate>Wed, 18 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/post/google-cloud/</guid>
      
        <description>&lt;p&gt;A guide to scaling machine learning projects using Google&#39;s tools.&lt;/p&gt;
&lt;h1 id=&#34;the-problem&#34;&gt;the problem&lt;/h1&gt;
&lt;p&gt;As I&#39;m working on a project with eye tracking data, here are some issues I&#39;m facing that you may relate to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The dataset I want to look at is increasing in volume (e.g. sample-by-sample data vs. aggregated values),&lt;/li&gt;
&lt;li&gt;I want to run multiple notebook sessions without my laptop/lab machine dying or being disconnected from a tool like Google Colab,&lt;/li&gt;
&lt;li&gt;Model runs and debugging is taking a long time due to above points,&lt;/li&gt;
&lt;li&gt;I want to be able to experiment with state-of-the-art tools to allow me to delve deeper into my data (e.g. computer vision or NLP tools to allow interpretability from raw data)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you relate to any of the above, you may find this post useful.&lt;/p&gt;
&lt;h1 id=&#34;background-reading&#34;&gt;background reading&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jupyter-notebook.readthedocs.io/en/stable/notebook.html&#34;&gt;Jupyter Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/notebooks/welcome.ipynb&#34;&gt;Google Colab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://developers.google.com/identity/protocols/oauth2&#34;&gt;Google API access and authentication&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/ai-platform/notebooks/docs/create-new&#34;&gt;Google AI Platform notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/storage/docs/key-terms#buckets&#34;&gt;Google Storage buckets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;workflow-for-researchers&#34;&gt;workflow for researchers&lt;/h1&gt;
&lt;p&gt;After working on a few of these projects, these are the areas for me that are labor intensive and where scaling specifically can help. Deliberately leaving out production or consumer interactions of models of here to focus on development and research.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/workflow.png&#34; alt=&#34;Data workflow for researchers&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;how-to-read-the-workflow-diagram&#34;&gt;how to read the workflow diagram&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Traditional projects have four components:
&lt;ul&gt;
&lt;li&gt;processing: where the bulk of &lt;strong&gt;computational&lt;/strong&gt; time and effort is spent training models, debugging, etc.&lt;/li&gt;
&lt;li&gt;analysis: where the bulk of &lt;strong&gt;human&lt;/strong&gt; time and effort is spent designing protocols, deriving meaning from results, etc.&lt;/li&gt;
&lt;li&gt;storage: the &lt;strong&gt;infrastructure&lt;/strong&gt; that needs to allow quick (and cheap) access to data,&lt;/li&gt;
&lt;li&gt;tools: to keep everything moving and involve others.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Most research&lt;/em&gt; can be done using the tools colored in &lt;strong&gt;green&lt;/strong&gt;, since it involves data that is smaller, or tools that are well-defined,&lt;/li&gt;
&lt;li&gt;&lt;em&gt;One way&lt;/em&gt; scaling is done is through expanding to the &lt;strong&gt;red&lt;/strong&gt; blocks, where methods like multiprocessing, TPUs and aggregating industry tools in a complex way can potentially improve research,&lt;/li&gt;
&lt;li&gt;Data flows to and from storage and other tools interface with them to track progress and feed in new experimental ideas, indicated by &lt;strong&gt;arrows&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It&#39;s more complicated than this, since researchers now connect local/Colab-type systems to access cloud APIs, send and receive data from multiple storage sources, and use a whole host of other tools I probably haven&#39;t heard of. For me, generally, I think visualization summarizes the workflow.&lt;/p&gt;
&lt;h1 id=&#34;advantages-to-google-cloud&#34;&gt;advantages to google cloud&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Faster storage access: buckets are extremely efficient, and I found downloading a 2 GB file from a bucket in Colab took over 4 minutes, whereas my default TF 2.0 AI Platform Notebook took 5 seconds. This is because of multiple CPUs available for cheap on Cloud,&lt;/li&gt;
&lt;li&gt;Stability: machines more rarely bug out on Cloud, since you can often allocate a virtual machine with slightly more resources than you will use,&lt;/li&gt;
&lt;li&gt;Access to Cloud APIs: yes, this is available locally and through Google Colab, but the Computer Vision or NLP APIs are just easier to use and work smoother within a Google Cloud VM,&lt;/li&gt;
&lt;li&gt;Scaling: you can more easily allocate CPUs, GPUs, TPUs, change server sites, or have end-to-end pipelines to scale existing data down the road within Cloud,&lt;/li&gt;
&lt;li&gt;More responsive: this comes with better allocation of resources and the host of multiprocessing tools built into accessing, processing and analyzing data.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;scaling&#34;&gt;scaling&lt;/h1&gt;
&lt;p&gt;The rest of this post will describe specific steps and references as I transition from higher-level tools designed for more broadly-applicable data workflows (&lt;strong&gt;green blocks&lt;/strong&gt;) to lower-level counterparts on Google Cloud (&lt;strong&gt;red blocks&lt;/strong&gt;). Please feel free to share your thoughts via email if you have suggestions!&lt;/p&gt;
&lt;h3 id=&#34;google-bucket-setup-and-initial-data-transfer&#34;&gt;google bucket setup and initial data transfer&lt;/h3&gt;
&lt;p&gt;I would recommend going all in and setting up Google Storage buckets. You can access data from Google Drive from within AI Platform, Colab and even locally.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/storage/docs/creating-buckets&#34;&gt;Create a storage bucket&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@philipplies/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041&#34;&gt;Transfer your files form Google Drive to a bucket&lt;/a&gt;. The provided link provides a Colab notebook with easy to use commands,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/storage/docs/gsutil/commands/rsync&#34;&gt;Use rsync commands as needed to sync this folder&lt;/a&gt;. You can get fancy with this and have a script that periodically syncs a bucket with you Drive folder (if you&#39;re using Drive to collaborate).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; storage cost is pretty cheap, but may add up depending on how large your bucket is.&lt;/p&gt;
&lt;h3 id=&#34;ai-platform-notebook-setup&#34;&gt;ai platform notebook setup&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/ai-platform/notebooks/docs/create-new#create_an_instance_with_default_properties&#34;&gt;Follow the Cloud tutorial to setup a notebook instance&lt;/a&gt;,&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You only start with a default engine for testing. Most likely you&#39;ll want the TF 2.0 version though if you are doing any model training.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; this creates a compute engine within Cloud. This is the main cost hog, unless you are adding GPUs or TPUs on top of this. When you start or stop an engine, it associates available resources to your instance, depending on what you requested. That&#39;s why the storage is &lt;strong&gt;persistent&lt;/strong&gt; but the computer resources are &lt;strong&gt;not&lt;/strong&gt; for an AI platform notebook.&lt;/p&gt;
&lt;h3 id=&#34;github-setup-for-analysis-notebooks&#34;&gt;github setup for analysis notebooks&lt;/h3&gt;
&lt;p&gt;I found it helpful to only leave notebooks within Github. All data should be downloaded from Google cloud, and analysis should be able to be done from scratching using the code in notebooks. This may be annoying, but I&#39;ve found it helpful to stick to it for replicability.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://help.github.com/en/github/getting-started-with-github/create-a-repo&#34;&gt;Create a new repo&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;You can simply go to &lt;em&gt;File -&amp;gt; Clone a Repo&lt;/em&gt; to clone a private (prompts login) or public repo. If you need more help with this, &lt;a href=&#34;https://cloud.google.com/ai-platform/notebooks/docs/save-to-github&#34;&gt;see here&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;Create your first notebook to import data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;data-transfer-protocol&#34;&gt;data transfer protocol&lt;/h3&gt;
&lt;p&gt;Data will first be imported into the Compute Engine that is associated with your AI Platform Notebook.&lt;/p&gt;
&lt;p&gt;I would recommend something like the following code block, with the following assumptions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;processing&lt;/strong&gt; is the folder where you store files downloaded from a bucket,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;import_folder&lt;/strong&gt; is the folder located in the bucket that will be used in the current notebook.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; os
&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exists(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;processing&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;):
    os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;makedirs(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;processing&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
    
&lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; google.colab
  IN_COLAB &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; True
&lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt;:
  IN_COLAB &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; False
&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; IN_COLAB &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exists(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;processing&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;):
    &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; google.colab &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; auth
    auth&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;authenticate_user()

    &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;gcloud config set project {project_id}
    &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;gsutil &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;m cp &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;r gs:&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; {import_folder} processing&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; IN_COLAB &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exists(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;processing&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;importing..&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
    &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;gsutil &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;m cp &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;r gs:&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;{import_folder} processing&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;done!&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;processing folder exists. please check that this is what you want.&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This chunk will allow a different type of authentication depending on whether the notebook is run in Colab or Google Cloud. If you want to potentially collaborate with someone in Colab, publish your code for replicability or because you don&#39;t want to deal with permissions, you want the notebook to be generalizable. The &lt;strong&gt;IN_COLAB&lt;/strong&gt; variable can be used to trigger analysis and processing in the same notebook, depending on where it&#39;s run.&lt;/p&gt;
&lt;h4 id=&#34;transferring-data-back-to-a-bucket&#34;&gt;transferring data back to a bucket&lt;/h4&gt;
&lt;p&gt;It may be the case you want to transfer data back to a bucket. I would advise against this type of workflow, and would recommend keeping only raw data within a bucket. Any data generated as a result of processing should be replicable, so regeneration through a notebook into a Compute Engine is the way to go.&lt;/p&gt;
&lt;p&gt;However, it may be possible that you want to store a model H5 file or processed data. For this use the following gsutil command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gsutil cp processing/upload_image.png gs://{upload_folder}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;strong&gt;upload_folder&lt;/strong&gt; may be the same as your &lt;strong&gt;import_folder&lt;/strong&gt;. &lt;strong&gt;upload_image.png&lt;/strong&gt; is variable, and you may need to add the &lt;strong&gt;-r&lt;/strong&gt; option if you want to upload a folder to your bucket.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/storage/docs/quickstart-gsutil#create&#34;&gt;See here for more interactions with the bucket&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;analysis&lt;/h3&gt;
&lt;p&gt;Now that we have some data processed in the AI platform, you may be interested in analyzing the results. You can either do this in the same notebook within the AI Platform, or on Colab.&lt;/p&gt;
&lt;p&gt;I prefer to always have a link to Colab. The &lt;a href=&#34;https://colab.research.google.com/drive/1Xc8E8mKC4MBvQ6Sw6akd_X5Z1cmHSNca&#34;&gt;aforementioned sample Colab&lt;/a&gt; has code that will transfer files between buckets and local Drive locations. It&#39;s highly dependent on your use case what you will transfer, but the relevant portions you want are below.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; google.colab &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; drive
drive&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mount(&lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/content/drive&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; google.colab &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; auth
auth&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;authenticate_user()

project_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;nifty-depth-246308&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;
&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;gcloud config set project {project_id}
&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;gsutil ls

bucket_name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;medium_demo_bucket_190710&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;

&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;gsutil &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;m cp &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;r gs:&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;{bucket_name}&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;analysis &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;content&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;drive&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;My\ Drive&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;Data&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this case, we connect Google Drive to Cloud, and copy files in the &lt;strong&gt;analysis&lt;/strong&gt; folder into a folder named &lt;strong&gt;data&lt;/strong&gt; on our Google Drive.&lt;/p&gt;
&lt;p&gt;The idea is that Colab is more replicable than working in Google Cloud, which is designed for faster processing and scability.&lt;/p&gt;
&lt;h1 id=&#34;lessons-learned&#34;&gt;lessons learned&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Monitor billing, and ask the support team if you&#39;re confused, they&#39;re pretty responsive,&lt;/li&gt;
&lt;li&gt;Try out free credit first, Cloud gives $300 for free.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;other-relevant-resources&#34;&gt;other relevant resources&lt;/h1&gt;
&lt;p&gt;-&lt;a href=&#34;https://github.com/GoogleCloudPlatform/ai-platform-samples&#34;&gt;See specific AI platform tutorials here&lt;/a&gt;&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>helpful (random) resources when learning eeg</title>
      <link>http://schko.github.io/post/eeg-resources/</link>
      <pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/post/eeg-resources/</guid>
      
        <description>&lt;p&gt;Part 3: Links and relevant literature relating to EEG in general and artifact correction.&lt;/p&gt;
&lt;h1 id=&#34;links&#34;&gt;links&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;The primary resource I found helpful is a &lt;a href=&#34;https://sccn.ucsd.edu/wiki/EEGLAB#The_EEGLAB_Tutorial_Outline&#34;&gt;tutorial from EEGLAB&lt;/a&gt;. Specifically, the single subject data processing which has a broad overview of processing EEG data.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;images&#34;&gt;images&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Headmap: &lt;a href=&#34;https://www.researchgate.net/figure/Head-map-This-figure-depicts-the-organization-of-the-EEG-electrode-sites_fig1_316856333&#34;&gt;https://www.researchgate.net/figure/Head-map-This-figure-depicts-the-organization-of-the-EEG-electrode-sites_fig1_316856333&lt;/a&gt;
&lt;img src=&#34;https://www.researchgate.net/profile/Eileen_Haebig/publication/316856333/figure/fig1/AS:505502740500480@1497532653203/Head-map-This-figure-depicts-the-organization-of-the-EEG-electrode-sites.png&#34; alt=&#34;Headmap&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Removing linear trends: &lt;a href=&#34;https://www.mathworks.com/help/matlab/data_analysis/detrending-data.html&#34;&gt;https://www.mathworks.com/help/matlab/data_analysis/detrending-data.html&lt;/a&gt;
&lt;img src=&#34;https://www.mathworks.com/help/examples/matlab/win64/RemovingLinearTrendsFromDataExample_02.png&#34; alt=&#34;removing trends&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data averaging methods and the visualizations they yeild: &lt;a href=&#34;https://sccn.ucsd.edu/wiki/Chapter_06:_Data_Averaging&#34;&gt;https://sccn.ucsd.edu/wiki/Chapter_06:_Data_Averaging&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;artifact-removal-and-correction&#34;&gt;artifact removal and correction&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Removing a mean &lt;strong&gt;baseline&lt;/strong&gt; value from each epoch is useful when baseline differences between data epochs (e.g., those arising from low frequency drifts or artifacts) are present. These are not meaningfully interpretable, but if left in the data could skew the data analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Removing &lt;strong&gt;linear trends&lt;/strong&gt; through high-pass filtering: &lt;a href=&#34;https://www.mathworks.com/help/matlab/data_analysis/detrending-data.html&#34;&gt;https://www.mathworks.com/help/matlab/data_analysis/detrending-data.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ICA&lt;/strong&gt; can be used to decompose data from either average reference, common reference, or bipolar reference channels &amp;ndash; or from more than one of these types at once. However, plotting single scalp maps requires that all channels use either the same common reference or the same average reference.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;EEG &lt;strong&gt;reference choice&lt;/strong&gt; has different effect on outcomes: &lt;a href=&#34;https://sapienlabs.org/effect-of-eeg-reference-choice-on-outcomes/&#34;&gt;https://sapienlabs.org/effect-of-eeg-reference-choice-on-outcomes/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Single trial analyses&lt;/strong&gt;: &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3210509/&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3210509/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;other-pointers&#34;&gt;other pointers&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;EEGLAB wiki on electrode locations and how signal is reported:&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: Equating &amp;lsquo;channel locations&amp;rsquo; with (single) electrode locations only makes sense when all channels use the same &amp;lsquo;reference channel.&amp;rsquo; An EEG channel signal is always the difference&amp;rsquo; between voltages at two (or more) electrodes &amp;ndash; typically some electrode &amp;ldquo;referred to&amp;rdquo; a reference channel (or channel combination. Equating the signal &amp;lsquo;channel location&amp;rsquo; to the location of one of the contributing electrodes is quite imprecise, as the channel must be equally sensitive to potentials flowing to either of its two (or more) contributing scalp electrodes.&lt;/p&gt;
&lt;/blockquote&gt;</description>
      
    </item>
    
    <item>
      <title>eye tracking in multimodal data</title>
      <link>http://schko.github.io/post/eeg-fmri-pupil-2/</link>
      <pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/post/eeg-fmri-pupil-2/</guid>
      
        <description>&lt;p&gt;Part 2: Mainly on eye tracking and pupillometry data.&lt;/p&gt;
&lt;h1 id=&#34;intention&#34;&gt;intention&lt;/h1&gt;
&lt;p&gt;Controlling for eye-related artifacts in EEG is a standard protocol, so I wanted to compile a list of the sources of electrical activity that result in distortion of EEG signal.&lt;/p&gt;
&lt;h1 id=&#34;we-may-not-need-to-remove-artifacts&#34;&gt;we may not need to remove artifacts&lt;/h1&gt;
&lt;p&gt;I outlined in my last post a couple of examples using deep learning models to either &amp;ldquo;learn noise&amp;rdquo; in order to remove it, or utilize noise with the end goal of assisting classification. There are similarities in how researchers use the word &amp;ldquo;noise&amp;rdquo; to how words like &amp;ldquo;artifacts&amp;rdquo; or &amp;ldquo;distortions&amp;rdquo; are also used, in my opinion.&lt;/p&gt;
&lt;p&gt;Including distortions in EEG may be essential to some research. A quick example of this is &lt;a href=&#34;https://github.com/tevisgehr/EEG-Classification&#34;&gt;Tevis Gher&#39;s project mentioned in the last post&lt;/a&gt;, where correction of blinking and other eye tracking related events would remove useful information that could aid in classification.&lt;/p&gt;
&lt;p&gt;I just mean to note that aggressive filtering or thresholding when finding independent components can lead to cases where we remove brain activity that affects the system of interest (e.g. EEG) and another that produces &amp;ldquo;artifacts&amp;rdquo; (e.g. blinking) to the detriment of the goal of a study (e.g. decision-making in a visual task where activity in certain regions of the brain affect blinking rates). In the parenthesized example of blink rate, it may be important to not remove blink artifacts when studying attention in children with ADHD, since Caplan et al &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; showed it may be a critical discriminatory measure between ADHD and non-ADHD populations.&lt;/p&gt;
&lt;h1 id=&#34;eye-related-artifacts&#34;&gt;eye related artifacts&lt;/h1&gt;
&lt;p&gt;With that out of the way, I wanted to summarize the types of corrections that seem to be most common in EEG processing, related to eye tracking data. This is based on (and the quotes below derive from) a very nice, controlled study by Plöchl, Ossandón and König in 2012 &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;artifact&lt;/th&gt;
&lt;th&gt;what is it&lt;/th&gt;
&lt;th&gt;what causes it (quotes from Plöchl et al)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;corneo-retinal dipole movement&lt;/td&gt;
&lt;td&gt;large ocular movements&lt;/td&gt;
&lt;td&gt;&lt;code&gt;orientation change of the eyeball and thus of the corneo-retinal dipole produced between the negatively charged retina and the positively charged cornea&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;blinks (eyelid-induced)&lt;/td&gt;
&lt;td&gt;spontaneous or willful blinking&lt;/td&gt;
&lt;td&gt;&lt;code&gt;eyelid slides down over the cornea, which is positively charged with respect to the forehead. Thereby the lid acts like a “sliding electrode,” short-circuiting the cornea to the scalp and producing artifacts in the EEG signal&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;eyelid-saccades&lt;/td&gt;
&lt;td&gt;saccade-accompanying ballistic eyelid movements&lt;/td&gt;
&lt;td&gt;&lt;code&gt;occur in synchrony with the rotation of the eyeball and therefore are not distinguishable from the corneo-retinal dipole offset in the raw data. During upward saccades, for instance, eyelid and eyeball move upwards with approximately the same speed. However, after the termination of both, eye- and eyelid saccades, the eyelid continues to slide more slowly for another 30–300 ms and produces a signal change that is observable particularly after upward saccades&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;saccadic spike potential&lt;/td&gt;
&lt;td&gt;potential appearing right before saccade onset&lt;/td&gt;
&lt;td&gt;depends on the type of eye movements. Characteristically, it&#39;s &lt;code&gt;a biphasic waveform, starting around 5ms prior to saccade onset and consisting ofa larger positive deflection followed by a smaller negative deflection&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;microsaccades&lt;/td&gt;
&lt;td&gt;involuntary, small eye movements that occur during fixations&lt;/td&gt;
&lt;td&gt;&lt;code&gt;microsaccade-induced confounds in the EEG are mainly caused by spike potentials occurring at microsaccade onset, while orientation changes of the corneo-retinal dipole only play a minor role&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;correction-of-eye-artifacts&#34;&gt;correction of eye artifacts&lt;/h1&gt;
&lt;p&gt;This will be covered in the next post.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/0006322395003150&#34;&gt;https://www.sciencedirect.com/science/article/pii/0006322395003150&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/23087632&#34;&gt;https://www.ncbi.nlm.nih.gov/pubmed/23087632&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
      
    </item>
    
    <item>
      <title>eeg, fmri, &amp; eye tracking</title>
      <link>http://schko.github.io/post/eeg-fmri-pupil/</link>
      <pubDate>Tue, 21 Jan 2020 14:49:07 -0500</pubDate>
      
      <guid>http://schko.github.io/post/eeg-fmri-pupil/</guid>
      
        <description>&lt;p&gt;There is growing interest in multimodal, physiological data collection for the purposes of understanding decision-making, cognitive performance and designing brain computer interfaces.&lt;/p&gt;
&lt;p&gt;Part 1: Mainly on EEG artifacts&lt;/p&gt;
&lt;h1 id=&#34;summary-of-data-sources&#34;&gt;summary of data sources&lt;/h1&gt;
&lt;p&gt;The source of EEG activity is post-synaptic activity of cortical pyramidal cells, and glial cell activity measured at the scalp. fMRI is derived from the measurement of blood oxygen changes in different regions of the brain. Eye tracking is the primary tool for measuring pupillometry and gaze.&lt;/p&gt;
&lt;h1 id=&#34;where-each-excel&#34;&gt;where each excel&lt;/h1&gt;
&lt;p&gt;Eye tracking excels in paradigms where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;knowing if stimuli was seen,&lt;/li&gt;
&lt;li&gt;modeling decision-making (i.e. &lt;strong&gt;behavior before events of interest&lt;/strong&gt;) is a priority,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;EEG excels where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;temporal resolution of brain activity is important,&lt;/li&gt;
&lt;li&gt;measuring &lt;strong&gt;reactivity to stimuli or events&lt;/strong&gt; of interest is necessary,&lt;/li&gt;
&lt;li&gt;interacting with an interface using brain activity.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;fMRI excels where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;spatial resolution &lt;strong&gt;in the context of an task&lt;/strong&gt; is important,&lt;/li&gt;
&lt;li&gt;connectivity of brain regions and how it plays a role in decision-making is important.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;artifact-removal-as-a-function-of-adding-on-data-sources&#34;&gt;artifact removal (as a function of adding on data sources)&lt;/h1&gt;
&lt;p&gt;EEG appears to be the type of data that is, not only most prone to signal variations by itself, most effected from adding on other sources. For example, gradient artifacts that result from the fMRI scanner&#39;s magnetic field overwhelm the raw EEG signals in amplitude to the point where you cannot use the same EEG pipelines as you would in unimodal data collection. Abreu et al&#39;s review is a great resource for a summary of recent (but notably, non-deep learning) methods in removing the effects of gradient artifacts from EEG &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/fig2_abreau.png&#34; alt=&#34;A visualization of how EEG can be bastardized with fMRI input, without artifact removal and filtering.&#34;&gt;&lt;/p&gt;
&lt;p&gt;I couldn&#39;t help but feel lost in the signal processing background literature, but it was interesting to take a step back to read a study where artifacts and noise filtering in EEG, for the most part, wasn&#39;t really needed. It was a part of a &lt;a href=&#34;https://github.com/tevisgehr/EEG-Classification&#34;&gt;poster presentation by Tevis Gher&lt;/a&gt; at UNL, and it highlights the potential in using deep learning techniques to avoid tedious signal processing pipelines, &lt;strong&gt;but only for classification problems&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The task is a discriminatory one, where a system classifies whether an EEG waveform derived from an individual who is visualizing a familiar vs. unfamiliar motor-based task. In this case, it&#39;s not really important to filter noise, artifacts, etc. A convolutional system is capable of ignoring variations in the signal that are not relevant to the task at hand. In some cases, the results used in the last stage (2D visualizations below) probably, meaningfully, use artifacts that may be problematic in other contexts. For example, filtering blinking may be preferred in most EEG contexts, but it may not be an &amp;ldquo;artifact&amp;rdquo; in a classification task such as this, if participants tend to blink more when imagining themselves doing a familiar task.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/eeg_architecture.png&#34; alt=&#34;A classification network used by Gher.&#34;&gt;&lt;/p&gt;
&lt;p&gt;In fact, &lt;a href=&#34;https://www.vision-systems.com/boards-software/article/14039375/fundamental-applications-of-deep-learning-networks&#34;&gt;image denoising has a practical use&lt;/a&gt; in computer vision, but (from an initial look) I did not see too much use of deep learning techniques for this purpose in cognition.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnhum.2018.00029/full&#34;&gt;https://www.frontiersin.org/articles/10.3389/fnhum.2018.00029/full&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</description>
      
    </item>
    
    <item>
      <title>extending a python application using django</title>
      <link>http://schko.github.io/post/web-dev-django/</link>
      <pubDate>Mon, 20 Jan 2020 12:53:20 -0500</pubDate>
      
      <guid>http://schko.github.io/post/web-dev-django/</guid>
      
        <description>&lt;p&gt;We recently demoed an &lt;a href=&#34;http://fractal.nyc/fovea&#34;&gt;eye tracking concept&lt;/a&gt; that uses gaze and pupil patterns to modify video in real time, but are in the process of making it work on any browser.&lt;/p&gt;
&lt;p&gt;This comes with challenges, including having communication channels between frontend (i.e. browser) and backend (i.e. Python). &lt;a href=&#34;https://www.djangoproject.com/&#34;&gt;Django&lt;/a&gt; makes this process smooth, but comes with a steep learning curve if you are not familiar with software development.&lt;/p&gt;
&lt;h1 id=&#34;a-helpful-tutorial&#34;&gt;a helpful tutorial&lt;/h1&gt;
&lt;p&gt;If you face a similar challenge: &lt;strong&gt;expanding a Python application to work with a web interface,&lt;/strong&gt; then I recommend you start with the &lt;a href=&#34;https://channels.readthedocs.io/en/latest/tutorial/index.html&#34;&gt;Django chat server application&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;CodingEntrepreneur&#39;s &lt;a href=&#34;https://www.youtube.com/watch?v=RVH05S1qab8&#34;&gt;WebSockets walk-through&lt;/a&gt; was especially useful if you want to understand how data is transferred throughout the Django framework.&lt;/p&gt;
&lt;p&gt;Some aspects we (so far) found most useful in this tutorial are:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;concept&lt;/th&gt;
&lt;th&gt;definition&lt;/th&gt;
&lt;th&gt;what it&#39;s used for in the tutorial&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;web server&lt;/td&gt;
&lt;td&gt;&lt;code&gt;software or hardware dedicated to running satisfying World Wide Web client requests&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;storage of web pages associated with running a chat room&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;caching&lt;/td&gt;
&lt;td&gt;&lt;code&gt;software component that stores data so that future requests for that data&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;speeding up accessing images or chats already seen&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;relational database&lt;/td&gt;
&lt;td&gt;&lt;code&gt;set of formally described tables from which data can be accessed or reassembled&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;storage of chats that can only be accessed by each user&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;object-oriented python backend&lt;/td&gt;
&lt;td&gt;&lt;code&gt;object&#39;s state changed only by it&#39;s behaviors, methods that actually validate and change the state&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;easily readable and modifiable models that work with html and js frontend&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;websockets&lt;/td&gt;
&lt;td&gt;&lt;code&gt;computer communications protocol, providing full-duplex communication channels over a single TCP connection&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;channels that allow transfer of data and commands between the front and backends&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;diagram-of-the-data-transfer&#34;&gt;diagram of the data transfer&lt;/h1&gt;
&lt;p&gt;Below is a very rough diagram of how all the parts fit together for the chat application, but which applies to a large number of problems.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://schko.github.io/img/chat_app_summary.jpg&#34; alt=&#34;Chat app Django framework&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-interesting-parts-from-the-diagram-above&#34;&gt;the interesting parts (from the diagram above)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The viewer &lt;strong&gt;views&lt;/strong&gt; file receives and adds to the &lt;strong&gt;Frontend&lt;/strong&gt;, e.g. viewing a chat screen and submitting a message,&lt;/li&gt;
&lt;li&gt;The Python &lt;strong&gt;views.py&lt;/strong&gt; file contributes to the HTML by loading a template, filling a context and returning an HttpResponse object that modifies the html,&lt;/li&gt;
&lt;li&gt;The HTML initializes and opens a websocket and defines its behavior (i.e. when it executes),&lt;/li&gt;
&lt;li&gt;Python communicates with the socket through the &lt;strong&gt;consumers.py&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;The user does something (e.g. submits a message) on the interface that is received by the consumers &lt;strong&gt;receive&lt;/strong&gt; function,&lt;/li&gt;
&lt;li&gt;The receive function does something (e.g. modifies the message), and sends back a message using the &lt;strong&gt;receive&lt;/strong&gt; message,&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;onmessage&lt;/strong&gt; modifies the HTML in some way (e.g. posts the new message into a form that is viewable by the viewer).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this way, we get communication between a viewer, website, server, and storage (usually accessed and defined through a relational database in the &lt;strong&gt;models&lt;/strong&gt; file).&lt;/p&gt;</description>
      
    </item>
    
    <item>
      <title>hugo resources</title>
      <link>http://schko.github.io/post/resources/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>http://schko.github.io/post/resources/</guid>
      
        <description>&lt;p&gt;Some basic Hugo resources that I keep coming back to.&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/xianmin/hugo-theme-jane&#34;&gt;Jane&lt;/a&gt; theme for this webpage.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.xianmin.org/hugo-theme-jane/post/jane-theme-preview/&#34;&gt;Markdown shortcuts&lt;/a&gt; for the Jane theme.&lt;/li&gt;
&lt;li&gt;Another great &lt;a href=&#34;https://sourceforge.net/p/hugo-generator/wiki/markdown_syntax&#34;&gt;Markdown shortcuts&lt;/a&gt; resource&lt;/li&gt;
&lt;li&gt;Summary of the &lt;a href=&#34;https://www.jakewiesler.com/blog/hugo-directory-structure/&#34;&gt;Hugo file structure&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</description>
      
    </item>
    
  </channel>
</rss>
